Not launched: train_output/40a82c64dfe9a2e552fd9bf6559eec08 ('ColoredMNIST', 'ERM', [0], 0)
Not launched: train_output/10a7c863fbee820202f4c33e7766cb39 ('ColoredMNIST', 'ERM', [0, 1], 0)
Not launched: train_output/ca621788c61f67aa385e1e765a35d984 ('ColoredMNIST', 'ERM', [0, 2], 0)
Not launched: train_output/cb8e9c13d29954c4c0453fda1be86270 ('ColoredMNIST', 'ERM', [1], 0)
Not launched: train_output/fa5fbfa9c09318d2847326472a52a8fd ('ColoredMNIST', 'ERM', [1, 2], 0)
Not launched: train_output/03598bac7f7f94d9e026ad5ee7e6c8cf ('ColoredMNIST', 'ERM', [2], 0)
Not launched: train_output/bff365b544e8d9cdb61b1b91452f7d54 ('ColoredMNIST', 'IRM', [0], 0)
Not launched: train_output/95e4e4bf68d698f896fbf9ea740ef75b ('ColoredMNIST', 'IRM', [0, 1], 0)
Not launched: train_output/735ed09a70ba3034fa0ca8b2bc68130c ('ColoredMNIST', 'IRM', [0, 2], 0)
Not launched: train_output/a5b94b7db84be19ccb9dc80f9e00d17c ('ColoredMNIST', 'IRM', [1], 0)
Not launched: train_output/3e5416cc23f84f8f7c1966e2a8160aee ('ColoredMNIST', 'IRM', [1, 2], 0)
Not launched: train_output/2187750a753111b1a542727da09eded1 ('ColoredMNIST', 'IRM', [2], 0)
Not launched: train_output/ba7b86c46e02e1745422e5cfb134d0af ('ColoredMNIST', 'AnticausalReg', [0], 0)
Not launched: train_output/0b25d9933e8e311ba40ccd3b2124ab6a ('ColoredMNIST', 'AnticausalReg', [0, 1], 0)
Not launched: train_output/6132e0ee363af56420d8284e0bf641bf ('ColoredMNIST', 'AnticausalReg', [0, 2], 0)
Not launched: train_output/f825dba6924b0f62aa92d6e561a9bd48 ('ColoredMNIST', 'AnticausalReg', [1], 0)
Not launched: train_output/3bbd0e3f41e1464551088bbfc341769c ('ColoredMNIST', 'AnticausalReg', [1, 2], 0)
Not launched: train_output/28f6badc4154f4f919f15760cd7debf8 ('ColoredMNIST', 'AnticausalReg', [2], 0)
18 jobs: 0 done, 0 incomplete, 18 not launched.
About to launch 18 jobs.
Are you sure? (y/n) Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/ca621788c61f67aa385e1e765a35d984
	save_model_every_checkpoint: False
	seed: 1272321174
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  0.0000000000  0.7332515121  0             0.2016825676 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  0.3428510205  0.5117731157  100           0.0119083071 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  0.6857020410  0.4812616548  200           0.0093039846 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  1.0285530616  0.4901591474  300           0.0092196298 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  1.3714040821  0.4958966559  400           0.0093577337 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  1.7142551026  0.4918114874  500           0.0089655852 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  2.0571061231  0.4776922196  600           0.0089714384 
0.8991322048  0.9007715388  0.8032892270  0.8073296185  0.0991589436  0.0947278183  2.3999571436  0.4686968759  700           0.0112808776 
0.8981144204  0.8996999571  0.8031820860  0.8071153022  0.1043552794  0.1013716245  2.7428081641  0.4670329830  800           0.0091783476 
0.8991322048  0.9007715388  0.8035035089  0.8075439348  0.1003910644  0.0960137162  3.0856591847  0.4451522872  900           0.0093295813 
0.8972573388  0.8975567938  0.8042534955  0.8071153022  0.1120158569  0.1058722675  3.4285102052  0.4499096456  1000          0.0111768603 
0.8947932291  0.8951993142  0.8028606632  0.8062580369  0.1166764879  0.1114444921  3.7713612257  0.4441113430  1100          0.0120719361 
0.8887400900  0.8915559366  0.8032356565  0.8058294042  0.1418010393  0.1345906558  4.1142122462  0.4394817677  1200          0.0112102771 
0.8960252839  0.8962708958  0.8046820592  0.8049721389  0.1163550651  0.1120874411  4.4570632667  0.4340875068  1300          0.0090266609 
0.8949003643  0.8966995285  0.8030749451  0.8039005572  0.1227835217  0.1213030433  4.7999142872  0.4337550938  1400          0.0097238874 
0.8459931433  0.8493356194  0.7890394814  0.7863266181  0.3139765361  0.3088298328  5.1427653078  0.4270366511  1500          0.0088446379 
0.8775444611  0.8778396914  0.7992714416  0.7993999143  0.1951572293  0.1937419631  5.4856163283  0.4339160457  1600          0.0094245505 
0.8608849368  0.8602657523  0.7947715219  0.7916845264  0.2570311244  0.2567509644  5.8284673488  0.4287099621  1700          0.0088392615 
0.8840261410  0.8797685384  0.8019499652  0.8004714959  0.1788182354  0.1757393913  6.1713183693  0.4208042002  1800          0.0087957406 
0.8786158132  0.8771967424  0.8029678041  0.7991855979  0.1949965179  0.2003857694  6.5141693898  0.4079522446  1900          0.0088696694 
0.8756160274  0.8797685384  0.8050570526  0.7957565366  0.1973536187  0.1982426061  6.8570204103  0.4134768976  2000          0.0086616278 
0.8401542747  0.8424774968  0.7981464617  0.7856836691  0.3476723630  0.3551221603  7.1998714309  0.4269358423  2100          0.0097231054 
0.8613134776  0.8647663952  0.8019499652  0.7912558937  0.2643167086  0.2593227604  7.5427224514  0.4224595723  2200          0.0104608464 
0.8612599100  0.8632661809  0.8074677238  0.7908272610  0.2712808700  0.2738962709  7.8855734719  0.4023720251  2300          0.0104731107 
0.8766338119  0.8761251607  0.8052713344  0.7940420060  0.1997642899  0.2006000857  8.2284244924  0.4188471720  2400          0.0091115236 
0.8633490465  0.8574796399  0.8073070124  0.7852550364  0.2617988964  0.2653236177  8.5712755129  0.4104721162  2500          0.0102870989 
0.8750267838  0.8776253751  0.8116462206  0.7981140163  0.2077998607  0.2102443206  8.9141265335  0.4010780379  2600          0.0109244895 
0.8902399829  0.8934847835  0.8058070392  0.8009001286  0.1396582204  0.1341620231  9.2569775540  0.4161764145  2700          0.0112652469 
0.8517784444  0.8542648950  0.8081105695  0.7876125161  0.3002089248  0.2951135877  9.5998285745  0.4146584123  2800          0.0114328122 
0.8469573602  0.8510501500  0.8005035624  0.7871838834  0.3194942947  0.3129018431  9.9426795950  0.3883969170  2900          0.0107914901 
0.8554210414  0.8572653236  0.8133069052  0.7908272610  0.2940483206  0.2880411487  10.285530615  0.4038268797  3000          0.0108061862 
0.8653310478  0.8696956708  0.8138961804  0.7957565366  0.2441206407  0.2404629233  10.628381636  0.4044022493  3100          0.0112916613 
0.8767409471  0.8739819974  0.8126104891  0.7959708530  0.1970857663  0.1935276468  10.971232656  0.3948981506  3200          0.0101974702 
0.8582065567  0.8570510073  0.8181818182  0.7914702100  0.2833342262  0.2801114445  11.314083677  0.4031227249  3300          0.0116761279 
0.8769552175  0.8726960994  0.8185568115  0.7959708530  0.2080141426  0.2085297900  11.656934697  0.3891585404  3400          0.0105589724 
0.8684915363  0.8656236605  0.8237531473  0.7912558937  0.2337815396  0.2293184741  11.999785718  0.3813127655  3500          0.0112519884 
0.8615277480  0.8594084869  0.8231103016  0.7901843120  0.2620667488  0.2631804544  12.342636738  0.3938386099  3600          0.0106327009 
0.8816155989  0.8801971710  0.8238067177  0.7972567510  0.1803717791  0.1785255036  12.685487759  0.3774073003  3700          0.0108252549 
0.8436897364  0.8386198028  0.8224674559  0.7871838834  0.3313333690  0.3291898843  13.028338779  0.3803318229  3800          0.0090356255 
0.8658131562  0.8675525075  0.8328601275  0.7953279040  0.2379600364  0.2321045864  13.371189800  0.3788563085  3900          0.0099736238 
0.8623312621  0.8662666095  0.8353243692  0.7927561080  0.2541383190  0.2486069438  13.714040820  0.3685005325  4000          0.0098378181 
0.8471180630  0.8493356194  0.8433599400  0.7781825975  0.3145122409  0.3101157308  14.056891841  0.3585625105  4100          0.0096865964 
0.8696164560  0.8677668238  0.8430385172  0.7916845264  0.2310494455  0.2301757394  14.399742861  0.3454966655  4200          0.0101041698 
0.8341547032  0.8317616802  0.8550383029  0.7753964852  0.3571007661  0.3521217317  14.742593882  0.3315792404  4300          0.0092496943 
0.8332440540  0.8315473639  0.8564311352  0.7766823832  0.3661541758  0.3681954565  15.085444902  0.3362833025  4400          0.0097624683 
0.8421898436  0.8454779254  0.8552525848  0.7854693528  0.3318155033  0.3311187312  15.428295923  0.3239180960  4500          0.0090904260 
0.8397257339  0.8444063438  0.8689666256  0.7760394342  0.3321904966  0.3306900986  15.771146943  0.3396247718  4600          0.0095139194 
0.8540282837  0.8568366910  0.8685380618  0.7818259751  0.2714951519  0.2839691384  16.113997964  0.3056378220  4700          0.0098401666 
0.8490464967  0.8540505787  0.8790378743  0.7794684955  0.2829592329  0.2906129447  16.456848984  0.2985240434  4800          0.0094621086 
0.8278872938  0.8294042006  0.8674130819  0.7678954136  0.3555472224  0.3559794256  16.799700005  0.2909134211  4900          0.0095279384 
0.8165845297  0.8227603943  0.8907698077  0.7651093013  0.3866180961  0.3829832833  17.142551025  0.2747907373  5000          0.0095575762 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: AnticausalReg
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/0b25d9933e8e311ba40ccd3b2124ab6a
	save_model_every_checkpoint: False
	seed: 1378205823
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.5108206557  0.5042863266  0.5001339262  0.5047149593  0.5054374029  0.4991427347  0.0000000000  439.49362182  0             0.2268610001 
0.5108206557  0.5042863266  0.5001339262  0.5047149593  0.5054374029  0.4991427347  0.3428510205  24.495810993  100           0.0154098511 
0.5107670881  0.5042863266  0.5001339262  0.5047149593  0.5054374029  0.4991427347  0.6857020410  1.6179377568  200           0.0161704183 
0.5108206557  0.5042863266  0.5001339262  0.5047149593  0.5054374029  0.4991427347  1.0285530616  1.5319773388  300           0.0151394272 
0.5106063853  0.5040720103  0.5001339262  0.5047149593  0.5053838324  0.4991427347  1.3714040821  1.4977224016  400           0.0152189517 
0.4271480609  0.4294899271  0.4385278834  0.4462066009  0.5621685327  0.5531504501  1.7142551026  1.4697163832  500           0.0147792172 
0.4063102636  0.4074153450  0.4240102855  0.4316330904  0.5893287620  0.5807972568  2.0571061231  1.4476729488  600           0.0162832785 
0.1859867152  0.1793827690  0.2683880645  0.2612516074  0.8621631757  0.8546935276  2.3999571436  1.4284787571  700           0.0158412099 
0.1391686308  0.1328761252  0.2276745058  0.2132447492  0.8563775647  0.8489069867  2.7428081641  1.4024456072  800           0.0162383127 
0.1026890936  0.0975139306  0.2045856324  0.1892413202  0.8994482241  0.8975567938  3.0856591847  1.3574966085  900           0.0163669968 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  3.4285102052  1.3148378789  1000          0.0155498934 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  3.7713612257  1.2571449745  1100          0.0161821294 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  4.1142122462  1.1909413171  1200          0.0159743237 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  4.4570632667  1.1596566665  1300          0.0174039435 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  4.7999142872  1.0990473408  1400          0.0162954402 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  5.1427653078  1.0619294626  1500          0.0162373304 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  5.4856163283  1.0312321728  1600          0.0162294054 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  5.8284673488  0.9981568533  1700          0.0174622369 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  6.1713183693  0.9850743473  1800          0.0164906073 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  6.5141693898  0.9724954069  1900          0.0163512063 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  6.8570204103  0.9441302502  2000          0.0153046227 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  7.1998714309  0.9308173323  2100          0.0162422919 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  7.5427224514  0.9050481009  2200          0.0165058517 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  7.8855734719  0.8942835140  2300          0.0149152088 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  8.2284244924  0.8807064670  2400          0.0153284454 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  8.5712755129  0.8486732864  2500          0.0147292638 
0.1022069852  0.0970852979  0.2040499277  0.1888126875  0.9010017678  0.8981997428  8.9141265335  0.8762801933  2600          0.0153853106 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  9.2569775540  0.8707197171  2700          0.0133400226 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  9.5998285745  0.8247989517  2800          0.0147381282 
0.1022069852  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  9.9426795950  0.8253488910  2900          0.0151788306 
0.1022069852  0.0970852979  0.2039963572  0.1888126875  0.9008946269  0.8984140592  10.285530615  0.8133984751  3000          0.0150911546 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  10.628381636  0.8091342252  3100          0.0155744600 
0.1022069852  0.0970852979  0.2039963572  0.1888126875  0.9010017678  0.8984140592  10.971232656  0.8351891506  3200          0.0153184748 
0.1022069852  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  11.314083677  0.7852995116  3300          0.0154784369 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  11.656934697  0.7901239449  3400          0.0152750468 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  11.999785718  0.7804801458  3500          0.0149089694 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  12.342636738  0.7702684486  3600          0.0158934331 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  12.685487759  0.7849671024  3700          0.0144666862 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  13.028338779  0.7659080327  3800          0.0146952319 
0.1022605528  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  13.371189800  0.7873071021  3900          0.0154715776 
0.1022605528  0.0970852979  0.2041034982  0.1888126875  0.9010017678  0.8984140592  13.714040820  0.7634399372  4000          0.0154531860 
0.1023141204  0.0970852979  0.2040499277  0.1888126875  0.9010017678  0.8981997428  14.056891841  0.7849471682  4100          0.0153947139 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  14.399742861  0.7678953716  4200          0.0150903201 
0.1021534176  0.0972996142  0.2039427867  0.1888126875  0.9008946269  0.8984140592  14.742593882  0.7465000671  4300          0.0157579327 
0.1023141204  0.0972996142  0.2041034982  0.1888126875  0.9008946269  0.8981997428  15.085444902  0.7791893291  4400          0.0151433229 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  15.428295923  0.7382348377  4500          0.0152816367 
0.1022605528  0.0972996142  0.2039963572  0.1888126875  0.9008410564  0.8984140592  15.771146943  0.7587073523  4600          0.0158965278 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  16.113997964  0.7489537188  4700          0.0130841398 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  16.456848984  0.7179356316  4800          0.0153122640 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  16.799700005  0.7369585773  4900          0.0141780996 
0.1021534176  0.0970852979  0.2039963572  0.1888126875  0.9009481974  0.8984140592  17.142551025  0.7262440574  5000          0.0156487370 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/10a7c863fbee820202f4c33e7766cb39
	save_model_every_checkpoint: False
	seed: 263935182
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.1066530962  0.0987998285  0.1986393100  0.1960994428  0.8894305459  0.8872696099  0.0000000000  0.7685459852  0             0.2197706699 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  0.3428510205  0.3397263721  100           0.0103808379 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  0.6857020410  0.3195532908  200           0.0109281158 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  1.0285530616  0.3343516007  300           0.0118582511 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  1.3714040821  0.3287101807  400           0.0111950302 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  1.7142551026  0.3252608736  500           0.0094189572 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  2.0571061231  0.3216642942  600           0.0098529172 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  2.3999571436  0.3116723691  700           0.0093115926 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  2.7428081641  0.3144788820  800           0.0096954107 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  3.0856591847  0.3097018765  900           0.0097899532 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  3.4285102052  0.3038591057  1000          0.0096492219 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  3.7713612257  0.3070896637  1100          0.0093224645 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  4.1142122462  0.2997824603  1200          0.0094503164 
0.1023676880  0.0936562366  0.1964429207  0.1926703815  0.8989125194  0.8964852122  4.4570632667  0.2959669863  1300          0.0091681218 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  4.7999142872  0.2998167095  1400          0.0094372368 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  5.1427653078  0.2901673654  1500          0.0096236205 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  5.4856163283  0.2975514233  1600          0.0097459650 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  5.8284673488  0.3039662120  1700          0.0096272206 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  6.1713183693  0.2926117229  1800          0.0095043635 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  6.5141693898  0.2962127222  1900          0.0097246432 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  6.8570204103  0.3006056951  2000          0.0095864511 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  7.1998714309  0.2870370474  2100          0.0098441529 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  7.5427224514  0.3014214985  2200          0.0095137596 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  7.8855734719  0.2736111858  2300          0.0097909045 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  8.2284244924  0.2955828850  2400          0.0094765067 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  8.5712755129  0.2864532401  2500          0.0093790770 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  8.9141265335  0.2863890165  2600          0.0096780467 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  9.2569775540  0.2734452106  2700          0.0098162103 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  9.5998285745  0.2755705739  2800          0.0097572708 
0.1022069852  0.0936562366  0.1964429207  0.1924560652  0.8989660899  0.8962708958  9.9426795950  0.2772412674  2900          0.0099159670 
0.1022605528  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  10.285530615  0.2872760469  3000          0.0103416729 
0.1022605528  0.0936562366  0.1963357797  0.1926703815  0.8989660899  0.8962708958  10.628381636  0.2773667800  3100          0.0095725489 
0.1030104993  0.0945135019  0.1970321959  0.1933133305  0.8990732308  0.8960565795  10.971232656  0.2717837349  3200          0.0097180676 
0.1022605528  0.0936562366  0.1963357797  0.1924560652  0.8989660899  0.8962708958  11.314083677  0.2613360882  3300          0.0094379711 
0.1031176345  0.0949421346  0.1977821825  0.1941705958  0.8988053785  0.8960565795  11.656934697  0.2687718091  3400          0.0094804072 
0.1023141204  0.0936562366  0.1962822092  0.1924560652  0.8989660899  0.8962708958  11.999785718  0.2655463812  3500          0.0098608327 
0.1029033640  0.0942991856  0.1970857663  0.1941705958  0.8988589489  0.8962708958  12.342636738  0.2676904173  3600          0.0100484896 
0.1027962288  0.0942991856  0.1967107730  0.1935276468  0.8990196604  0.8960565795  12.685487759  0.2601496348  3700          0.0097945333 
0.1065995286  0.0983711959  0.2004607061  0.1971710244  0.8992339423  0.8958422632  13.028338779  0.2622280737  3800          0.0092425108 
0.1217591601  0.1185169310  0.2129961965  0.2117445349  0.8992875127  0.8926275182  13.371189800  0.2485560197  3900          0.0097390842 
0.1180629955  0.1110158594  0.2094605453  0.2046720960  0.8992339423  0.8932704672  13.714040820  0.2565682034  4000          0.0095014000 
0.1052067709  0.0960137162  0.1988000214  0.1963137591  0.8994482241  0.8962708958  14.056891841  0.2508125578  4100          0.0094402504 
0.1267409471  0.1187312473  0.2163711362  0.2147449636  0.9016446135  0.8926275182  14.399742861  0.2659632458  4200          0.0095122051 
0.1343475466  0.1255893699  0.2234960090  0.2175310759  0.9033588686  0.8932704672  14.742593882  0.2610285098  4300          0.0100281191 
0.1354188987  0.1249464209  0.2237638614  0.2198885555  0.9049659828  0.8919845692  15.085444902  0.2351630270  4400          0.0097770834 
0.2234304693  0.2190312902  0.3005839181  0.2914702100  0.9051266942  0.8662666095  15.428295923  0.2467377731  4500          0.0100092340 
0.2009320763  0.1937419631  0.2793164408  0.2736819546  0.9025553115  0.8722674668  15.771146943  0.2259824435  4600          0.0099669123 
0.1595243197  0.1540934419  0.2438527883  0.2419631376  0.9111801575  0.8834119160  16.113997964  0.2253505109  4700          0.0096497297 
0.1335440326  0.1253750536  0.2216746130  0.2198885555  0.9118230032  0.8894127733  16.456848984  0.2417414238  4800          0.0097036862 
0.1359545747  0.1290184312  0.2235495795  0.2201028718  0.9129479831  0.8911273039  16.799700005  0.2204794019  4900          0.0100199556 
0.1475787444  0.1373767681  0.2307280227  0.2327475354  0.9192157283  0.8857693956  17.142551025  0.2059465278  5000          0.0101182246 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: AnticausalReg
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/6132e0ee363af56420d8284e0bf641bf
	save_model_every_checkpoint: False
	seed: 460406950
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.1061709878  0.1013716245  0.2071570151  0.1894556365  0.8848770558  0.8861980283  0.0000000000  448.28140258  0             0.3087301254 
0.8727233769  0.8776253751  0.7831467295  0.7974710673  0.1648363422  0.1714530647  0.3428510205  16.912056580  100           0.0169874763 
0.5808335119  0.5788684098  0.5674720094  0.5655807973  0.4669738040  0.4627089584  0.6857020410  1.5920661294  200           0.0173786259 
0.8096207414  0.8193313330  0.7406117748  0.7552507501  0.2724058499  0.2867552508  1.0285530616  1.5078615344  300           0.0183360195 
0.8857403043  0.8915559366  0.7901108909  0.8015430776  0.1381046767  0.1350192885  1.3714040821  1.4707782400  400           0.0173406029 
0.8425648168  0.8446206601  0.7641292120  0.7820402915  0.2489955537  0.2539648521  1.7142551026  1.4442351699  500           0.0177261329 
0.9010070709  0.9078439777  0.7992178711  0.8161165881  0.1046767022  0.1041577368  2.0571061231  1.4221708190  600           0.0172389221 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028553062  0.1013716245  2.3999571436  1.4001828110  700           0.0141827941 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  2.7428081641  1.3842437863  800           0.0172337985 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  3.0856591847  1.3657133949  900           0.0168337488 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  3.4285102052  1.3453206754  1000          0.0179891014 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  3.7713612257  1.3233598542  1100          0.0176402378 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  4.1142122462  1.3142380762  1200          0.0172499967 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  4.4570632667  1.2866533053  1300          0.0185625196 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  4.7999142872  1.3015145957  1400          0.0176197600 
0.9017034498  0.9082726104  0.7989500187  0.8165452207  0.1028017357  0.1011573082  5.1427653078  1.2793579888  1500          0.0180784106 
0.9017034498  0.9084869267  0.7990035892  0.8165452207  0.1028017357  0.1013716245  5.4856163283  1.2710805500  1600          0.0177491045 
0.9015963145  0.9082726104  0.7991643006  0.8169738534  0.1030160176  0.1013716245  5.8284673488  1.2478955114  1700          0.0174107981 
0.9000964217  0.9076296614  0.7995928644  0.8144020574  0.1071945144  0.1069438491  6.1713183693  1.2568038654  1800          0.0177812839 
0.9006856653  0.9067723961  0.7986285959  0.8154736391  0.1052659774  0.1041577368  6.5141693898  1.2419617426  1900          0.0165767646 
0.9008999357  0.9072010287  0.7990035892  0.8156879554  0.1054266888  0.1035147878  6.8570204103  1.2383930266  2000          0.0156506538 
0.8926505250  0.8971281612  0.7940215353  0.8066866695  0.1306583811  0.1311615945  7.1998714309  1.2122915494  2100          0.0151399875 
0.8980608528  0.9044149164  0.7975571865  0.8122588941  0.1136765415  0.1116588084  7.5427224514  1.1738078344  2200          0.0180276012 
0.9006856653  0.9078439777  0.7990035892  0.8154736391  0.1049445546  0.1045863695  7.8855734719  1.1884054053  2300          0.0177497959 
0.9005249625  0.9072010287  0.7984678845  0.8144020574  0.1064445278  0.1052293185  8.2284244924  1.1590539908  2400          0.0157177377 
0.9012213413  0.9078439777  0.7991643006  0.8159022718  0.1045159908  0.1033004715  8.5712755129  1.1605904907  2500          0.0151647735 
0.9014891793  0.9082726104  0.7987357369  0.8163309044  0.1034445813  0.1022288898  8.9141265335  1.1554304302  2600          0.0191971350 
0.8994536105  0.9063437634  0.7976643274  0.8161165881  0.1115337226  0.1133733390  9.2569775540  1.1359197718  2700          0.0125246835 
0.8971502036  0.9037719674  0.7977714684  0.8120445778  0.1174264745  0.1187312473  9.5998285745  1.1239512783  2800          0.0168579721 
0.8505999571  0.8574796399  0.7759682863  0.7803257608  0.2665666685  0.2666095156  9.9426795950  1.1202589101  2900          0.0172282410 
0.9002571245  0.9072010287  0.7987893073  0.8139734248  0.1072480849  0.1067295328  10.285530615  1.1395819318  3000          0.0159146452 
0.9014891793  0.9072010287  0.7985750254  0.8161165881  0.1041409975  0.1037291042  10.628381636  1.0952940691  3100          0.0158165383 
0.8899185772  0.8986283755  0.7961643542  0.8058294042  0.1415867574  0.1442348907  10.971232656  1.0983646363  3200          0.0179768205 
0.8989715020  0.9052721817  0.7984678845  0.8137591084  0.1120694273  0.1101585941  11.314083677  1.1066600072  3300          0.0188406682 
0.9002571245  0.9072010287  0.7990571597  0.8150450064  0.1064445278  0.1065152165  11.656934697  1.0932361931  3400          0.0189432478 
0.8889007928  0.8930561509  0.7958429314  0.8092584655  0.1507473081  0.1545220746  11.999785718  1.1008187932  3500          0.0184149098 
0.8973109064  0.9044149164  0.7990035892  0.8122588941  0.1174800450  0.1191598800  12.342636738  1.0806568366  3600          0.0166550493 
0.8834368974  0.8876982426  0.7938608239  0.8021860266  0.1737290406  0.1780968710  12.685487759  1.0578617465  3700          0.0176498723 
0.8950610671  0.9001285898  0.7985750254  0.8096870982  0.1277120051  0.1283754822  13.028338779  1.0728622729  3800          0.0179952931 
0.8980608528  0.9039862838  0.7986285959  0.8131161595  0.1145872395  0.1146592370  13.371189800  1.0715650845  3900          0.0174295807 
0.8968287979  0.9009858551  0.7982000321  0.8120445778  0.1211228371  0.1217316760  13.714040820  1.0825801367  4000          0.0182698274 
0.9002571245  0.9072010287  0.7985214550  0.8146163738  0.1069266620  0.1054436348  14.056891841  1.0421874356  4100          0.0182368207 
0.8915791729  0.8981997428  0.7961643542  0.8064723532  0.1442652810  0.1450921560  14.399742861  1.0373623300  4200          0.0180753708 
0.9011677737  0.9074153450  0.7986821664  0.8146163738  0.1051052660  0.1052293185  14.742593882  1.0752624047  4300          0.0188888502 
0.8928647954  0.8964852122  0.7972893341  0.8060437205  0.1408903412  0.1414487784  15.085444902  1.0557709205  4400          0.0189075994 
0.8959717163  0.9012001715  0.7987357369  0.8116159451  0.1248191997  0.1255893699  15.428295923  1.0534978926  4500          0.0172349977 
0.8972573388  0.9044149164  0.7984678845  0.8118302615  0.1188193068  0.1202314616  15.771146943  1.0722521484  4600          0.0135744691 
0.8925433898  0.8973424775  0.7971286227  0.8107586798  0.1408903412  0.1459494213  16.113997964  1.0482625079  4700          0.0151814651 
0.8990786372  0.9046292327  0.7989500187  0.8135447921  0.1143729576  0.1148735534  16.456848984  1.0549730819  4800          0.0147211528 
0.8949539319  0.9018431204  0.7988428778  0.8094727818  0.1300691059  0.1326618088  16.799700005  1.0448334706  4900          0.0146909571 
0.8444932505  0.8493356194  0.7770396957  0.7914702100  0.2989232335  0.3004714959  17.142551025  1.0396206343  5000          0.0143841529 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/735ed09a70ba3034fa0ca8b2bc68130c
	save_model_every_checkpoint: False
	seed: 1694510477
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          nll           penalty       step          step_time    
0.8992929076  0.8906986712  0.7964322066  0.8045435062  0.1037660042  0.0975139306  0.0000000000  0.7794597745  0.7722355723  0.0072242152  0             0.2063016891 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  0.3428510205  0.5238228098  0.5184507611  0.0053720476  100           0.0150947666 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  0.6857020410  0.4896187589  0.4898220733  -0.000203316  200           0.0148912549 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  1.0285530616  0.5124106845  0.5105014473  0.0019092343  300           0.0151867914 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  1.3714040821  0.5139052373  0.5121638662  0.0017413734  400           0.0149590611 
0.6784872509  0.6624517788  0.6369529116  0.6234462066  0.3800289281  0.3780540077  1.7142551026  0.5012093127  0.4987808698  0.0008792935  500           0.0158342505 
0.5009642168  0.5098585512  0.4999196443  0.5240034291  0.5053838324  0.5042863266  2.0571061231  0.8138394514  0.6653997904  0.0014843966  600           0.0162453389 
0.5009642168  0.5098585512  0.4999196443  0.5240034291  0.5053838324  0.5042863266  2.3999571436  0.6952100855  0.6636368263  0.0003157326  700           0.0166049790 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  2.7428081641  0.7387267336  0.6018557060  0.0013687103  800           0.0162766433 
0.5073923291  0.5147878268  0.5039374297  0.5261465924  0.4883484223  0.4867123875  3.0856591847  0.7659244356  0.5567143247  0.0020921011  900           0.0159818792 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  3.4285102052  0.6178624937  0.6015869704  0.0001627552  1000          0.0169866991 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  3.7713612257  0.6801426801  0.5747803834  0.0010536230  1100          0.0168483758 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  4.1142122462  0.5870252490  0.5488115266  0.0003821372  1200          0.0165117836 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  4.4570632667  0.5357796517  0.5549343169  -0.000191546  1300          0.0161143064 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  4.7999142872  0.5486167791  0.5164413685  0.0003217541  1400          0.0159804773 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  5.1427653078  0.6865712859  0.5187808001  0.0016779048  1500          0.0164025736 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  5.4856163283  0.5612470454  0.5137533265  0.0004749372  1600          0.0148257947 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  5.8284673488  0.4655332103  0.4954913461  -0.000299581  1700          0.0150470948 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  6.1713183693  0.6094154954  0.5155708659  0.0009384463  1800          0.0147817898 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  6.5141693898  0.5348404443  0.5133516404  0.0002148881  1900          0.0145295429 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  6.8570204103  0.5477988310  0.5178225046  0.0002997633  2000          0.0152588058 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  7.1998714309  0.4774954093  0.5140219644  -0.000365265  2100          0.0154258442 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  7.5427224514  0.6343474871  0.5141339600  0.0012021353  2200          0.0144435501 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  7.8855734719  0.6330024594  0.5150701058  0.0011793235  2300          0.0126764107 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  8.2284244924  0.4392066963  0.5053250048  -0.000661183  2400          0.0121972823 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  8.5712755129  0.6222762969  0.5120167398  0.0011025956  2500          0.0127427077 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  8.9141265335  0.6306799418  0.5244836992  0.0010619624  2600          0.0126280951 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  9.2569775540  0.5057039058  0.5030813885  0.0000262252  2700          0.0129389739 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  9.5998285745  0.5421329193  0.5038439757  0.0003828894  2800          0.0129031730 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  9.9426795950  0.4883579947  0.5026336268  -0.000142756  2900          0.0122636175 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  10.285530615  0.5943706726  0.5054810560  0.0008888962  3000          0.0130230331 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  10.628381636  0.5198186517  0.5048615763  0.0001495707  3100          0.0129082751 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  10.971232656  0.6196506013  0.5061018497  0.0011354875  3200          0.0126036954 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  11.314083677  0.5027560350  0.5024000433  0.0000035599  3300          0.0120240879 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  11.656934697  0.4617283557  0.5121129149  -0.000503845  3400          0.0119841266 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  11.999785718  0.6646129107  0.5136528036  0.0015096010  3500          0.0122007275 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  12.342636738  0.4457540400  0.5033834401  -0.000576294  3600          0.0126008177 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  12.685487759  0.4501363069  0.5101289824  -0.000599926  3700          0.0125386715 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  13.028338779  0.5889044254  0.5133274880  0.0007557694  3800          0.0128326249 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  13.371189800  0.7553467050  0.5046641648  0.0025068255  3900          0.0124412370 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  13.714040820  0.5004434744  0.5052347219  -0.000047912  4000          0.0126069593 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  14.056891841  0.5990427034  0.5073677841  0.0009167492  4100          0.0134353209 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  14.399742861  0.4178881872  0.5051989737  -0.000873107  4200          0.0131491184 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  14.742593882  0.5026259950  0.5137431431  -0.000111171  4300          0.0143563914 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  15.085444902  0.4319344375  0.5003917778  -0.000684573  4400          0.0141824412 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  15.428295923  0.3037302166  0.5157201037  -0.002119898  4500          0.0136037016 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  15.771146943  0.5945244487  0.5089260650  0.0008559837  4600          0.0140906286 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  16.113997964  0.3623624527  0.5035014808  -0.001411390  4700          0.0145713210 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  16.456848984  0.4626405618  0.5115601480  -0.000489196  4800          0.0138949776 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  16.799700005  0.6141105370  0.5014459202  0.0011266462  4900          0.0138851666 
0.9024533962  0.8932704672  0.7985750254  0.8075439348  0.1035517223  0.0966566652  17.142551025  0.6233617888  0.5014097488  0.0012195205  5000          0.0139089847 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/fa5fbfa9c09318d2847326472a52a8fd
	save_model_every_checkpoint: False
	seed: 650845313
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.4889650739  0.5006429490  0.4868484491  0.5030004286  0.4956875770  0.5036433776  0.0000000000  0.6921746731  0             0.3020694256 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  0.3428510205  0.3485982637  100           0.0127811146 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  0.6857020410  0.3251705408  200           0.0124675822 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  1.0285530616  0.3323789246  300           0.0131239891 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  1.3714040821  0.3300123537  400           0.0130890942 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  1.7142551026  0.3298740153  500           0.0125648952 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  2.0571061231  0.3214620472  600           0.0128097773 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  2.3999571436  0.3112204191  700           0.0124300051 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  2.7428081641  0.3224645478  800           0.0116993809 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  3.0856591847  0.3221681219  900           0.0122001696 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  3.4285102052  0.3128813621  1000          0.0103190207 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  3.7713612257  0.3059700211  1100          0.0099675798 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  4.1142122462  0.3184687793  1200          0.0098323750 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  4.4570632667  0.3105462727  1300          0.0110440016 
0.8978465824  0.8928418345  0.8001285691  0.7998285469  0.0978196818  0.1033004715  4.7999142872  0.3081427526  1400          0.0100348759 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  5.1427653078  0.2970150642  1500          0.0110724306 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  5.4856163283  0.3121394786  1600          0.0110530257 
0.8975787444  0.8930561509  0.8002357101  0.7993999143  0.0980875342  0.1035147878  5.8284673488  0.2974199588  1700          0.0095489073 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  6.1713183693  0.3020569661  1800          0.0095994806 
0.8977930148  0.8928418345  0.8001285691  0.7998285469  0.0985696684  0.1037291042  6.5141693898  0.2984945135  1900          0.0095443249 
0.8977930148  0.8930561509  0.8001821396  0.7996142306  0.0981411046  0.1035147878  6.8570204103  0.3104829013  2000          0.0095996165 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  7.1998714309  0.3037819355  2100          0.0090922618 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  7.5427224514  0.2935295011  2200          0.0090735412 
0.8973109064  0.8917702529  0.8003428510  0.7989712816  0.1013553329  0.1067295328  7.8855734719  0.2956669345  2300          0.0087217379 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  8.2284244924  0.2967845438  2400          0.0090254474 
0.8978465824  0.8928418345  0.8001821396  0.7998285469  0.0980875342  0.1037291042  8.5712755129  0.3018602395  2500          0.0090722251 
0.8976858796  0.8930561509  0.8002357101  0.7993999143  0.0984089570  0.1048006858  8.9141265335  0.3099327523  2600          0.0090780449 
0.8982215556  0.8906986712  0.7983071731  0.7991855979  0.1058016821  0.1101585941  9.2569775540  0.2760908608  2700          0.0091509366 
0.8978465824  0.8930561509  0.8001821396  0.7998285469  0.0978732523  0.1035147878  9.5998285745  0.2877750288  2800          0.0096507049 
0.8978465824  0.8930561509  0.8001821396  0.7993999143  0.0979268227  0.1037291042  9.9426795950  0.2901033875  2900          0.0090783787 
0.8982751232  0.8915559366  0.7994857235  0.7991855979  0.1043552794  0.1110158594  10.285530615  0.2817336373  3000          0.0093905330 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  10.628381636  0.2911498384  3100          0.0108061624 
0.8979001500  0.8930561509  0.8001821396  0.7998285469  0.0978196818  0.1037291042  10.971232656  0.2833759969  3200          0.0104508591 
0.8980608528  0.8919845692  0.7998607168  0.7993999143  0.1005517759  0.1060865838  11.314083677  0.2855581169  3300          0.0108019114 
0.8978465824  0.8930561509  0.8001821396  0.7996142306  0.0978196818  0.1033004715  11.656934697  0.2690727173  3400          0.0098978090 
0.8979537176  0.8926275182  0.8000214282  0.7991855979  0.1009803396  0.1075867981  11.999785718  0.2812283687  3500          0.0106632376 
0.8987036640  0.8926275182  0.7997000054  0.7978997000  0.1031231585  0.1084440634  12.342636738  0.2682623320  3600          0.0124424911 
0.8978465824  0.8904843549  0.7999678577  0.7976853836  0.1054802593  0.1120874411  12.685487759  0.2790738819  3700          0.0127223015 
0.8983822584  0.8921988856  0.8001285691  0.7989712816  0.0993732255  0.1056579511  13.028338779  0.2715448341  3800          0.0128429842 
0.8980072852  0.8926275182  0.8001821396  0.7998285469  0.0986232389  0.1043720532  13.371189800  0.2747638955  3900          0.0115646791 
0.8979001500  0.8928418345  0.8003964215  0.7996142306  0.0990518026  0.1043720532  13.714040820  0.2668997963  4000          0.0123129559 
0.8983822584  0.8911273039  0.7999142872  0.7981140163  0.1037124337  0.1093013288  14.056891841  0.2750372412  4100          0.0120766997 
0.8989715020  0.8913416202  0.7998071463  0.7983283326  0.1023731719  0.1080154308  14.399742861  0.2794396676  4200          0.0119654417 
0.9004178273  0.8883411916  0.7993785825  0.7985426489  0.1137301120  0.1228032576  14.742593882  0.2699967650  4300          0.0119119525 
0.8972573388  0.8831975997  0.7981464617  0.7942563223  0.1283012803  0.1371624518  15.085444902  0.2719908105  4400          0.0120542240 
0.8989715020  0.8891984569  0.7990571597  0.7957565366  0.1123372797  0.1178739820  15.428295923  0.2588034029  4500          0.0138466716 
0.9034711806  0.8874839263  0.7978786093  0.7912558937  0.1211764076  0.1290184312  15.771146943  0.2536038940  4600          0.0122724056 
0.9050782087  0.8829832833  0.7977178979  0.7916845264  0.1388010928  0.1435919417  16.113997964  0.2487385000  4700          0.0120806551 
0.9038997214  0.8825546507  0.7959500723  0.7931847407  0.1377296834  0.1423060437  16.456848984  0.2278972786  4800          0.0122907901 
0.9051317763  0.8894127733  0.7990571597  0.7981140163  0.1149622328  0.1215173596  16.799700005  0.2335738942  4900          0.0118341517 
0.9093100493  0.8795542220  0.7946643810  0.7923274754  0.1569079124  0.1654522075  17.142551025  0.2303160622  5000          0.0116784263 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: AnticausalReg
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/ba7b86c46e02e1745422e5cfb134d0af
	save_model_every_checkpoint: False
	seed: 647302116
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.5022498393  0.5096442349  0.5036695773  0.5113587655  0.5182943162  0.5055722246  0.0000000000  532.45910644  0             0.3249452114 
0.5059460039  0.5147878268  0.5058123962  0.5135019288  0.5140086784  0.5030004286  0.3428510205  22.843777422  100           0.0298072529 
0.5061602743  0.5152164595  0.5060266781  0.5135019288  0.5140086784  0.5025717960  0.6857020410  1.6210262418  200           0.0314910460 
0.5703878294  0.5576510930  0.5706326673  0.5820831547  0.6176675417  0.6112301757  1.0285530616  1.5320165169  300           0.0308450890 
0.5035354618  0.5111444492  0.5042052820  0.5113587655  0.5169550544  0.5060008573  1.3714040821  1.4944812524  400           0.0302068567 
0.5037497322  0.5115730819  0.5039374297  0.5120017145  0.5170086248  0.5064294899  1.7142551026  1.4655979002  500           0.0305295587 
0.5051960574  0.5135019288  0.5051695505  0.5117873982  0.5148122355  0.5027861123  2.0571061231  1.4526238525  600           0.0307711196 
0.5028926505  0.5107158165  0.5051695505  0.5128589799  0.5260084641  0.5150021432  2.3999571436  1.4408347642  700           0.0305964160 
0.5088922220  0.5165023575  0.5169014839  0.5237891127  0.5569721969  0.5460780111  2.7428081641  1.4320305848  800           0.0295549321 
0.5038568674  0.5124303472  0.5052766915  0.5137162452  0.5229549472  0.5117873982  3.0856591847  1.4217741406  900           0.0280490041 
0.5059460039  0.5094299186  0.5145443831  0.5216459494  0.5665077409  0.5565795114  3.4285102052  1.4155185151  1000          0.0279655409 
0.5407649454  0.5280754393  0.5570257674  0.5580797257  0.6677023625  0.6671667381  3.7713612257  1.4106267345  1100          0.0260641837 
0.5479965717  0.5323617660  0.5618471099  0.5619374196  0.6553275834  0.6508786970  4.1142122462  1.4042712343  1200          0.0264630651 
0.5327833726  0.5274324904  0.5505437403  0.5495070724  0.6895055445  0.6858122589  4.4570632667  1.3985187972  1300          0.0269417787 
0.5454788944  0.5300042863  0.5609364118  0.5600085727  0.6836127926  0.6798114016  4.7999142872  1.3967049706  1400          0.0254052162 
0.5512641954  0.5370767252  0.5660791772  0.5645092156  0.6891305512  0.6821688813  5.1427653078  1.3920344293  1500          0.0272812557 
0.5414613242  0.5293613373  0.5592757272  0.5621517360  0.6860770343  0.6780968710  5.4856163283  1.3910158348  1600          0.0266970849 
0.5362652668  0.5255036434  0.5578293245  0.5525075011  0.7100766058  0.7040291470  5.8284673488  1.3878707623  1700          0.0268223643 
0.5715663167  0.5621517360  0.5857395404  0.5827261037  0.6912733701  0.6896699529  6.1713183693  1.3829451811  1800          0.0289593792 
0.5432290551  0.5368624089  0.5662934590  0.5636519503  0.7190764451  0.7085297900  6.5141693898  1.3819811618  1900          0.0266227913 
0.5746196700  0.5636519503  0.5885787754  0.5880840120  0.6896662560  0.6834547793  6.8570204103  1.3862438059  2000          0.0279455757 
0.5593529034  0.5510072868  0.5782932448  0.5728675525  0.7147908073  0.7091727390  7.1998714309  1.3832367921  2100          0.0281042600 
0.6038140133  0.5985855122  0.6083998500  0.6142306044  0.6550061606  0.6412344621  7.5427224514  1.3827331924  2200          0.0273914194 
0.4873580459  0.4811401629  0.5102051749  0.5094299186  0.6805057053  0.6738105444  7.8855734719  1.3731325805  2300          0.0275744653 
0.5669595029  0.5645092156  0.5819896073  0.5792970424  0.6728986982  0.6643806258  8.2284244924  1.4137955725  2400          0.0275883007 
0.5642275552  0.5677239606  0.5811860502  0.5799399914  0.7166657738  0.7061723103  8.5712755129  1.3765558743  2500          0.0282844687 
0.5645489608  0.5642948993  0.5852038356  0.5816545221  0.7206299888  0.7115302186  8.9141265335  1.3768838084  2600          0.0295460153 
0.5663702593  0.5548649807  0.5834360101  0.5840120017  0.7299512509  0.7175310759  9.2569775540  1.3746081805  2700          0.0295789003 
0.5547460896  0.5475782255  0.5739540365  0.5745820832  0.7097551829  0.7012430347  9.5998285745  1.3762783349  2800          0.0263496566 
0.5703342618  0.5612944706  0.5910965876  0.5885126447  0.7250763379  0.7188169739  9.9426795950  1.3697836363  2900          0.0288483381 
0.5836725948  0.5792970424  0.5953822253  0.5893699100  0.6612203354  0.6513073296  10.285530615  1.3570072067  3000          0.0292413306 
0.5516391686  0.5398628375  0.5717040767  0.5707243892  0.7309155194  0.7228889841  10.628381636  1.3738551116  3100          0.0296724749 
0.5682986930  0.5687955422  0.5891144801  0.5812258894  0.7385760969  0.7301757394  10.971232656  1.3539342391  3200          0.0290927958 
0.5906363831  0.5865837977  0.6087748433  0.6000857265  0.7285048481  0.7241748821  11.314083677  1.3871451640  3300          0.0285888195 
0.6222948361  0.6150878697  0.6290244817  0.6320188598  0.7089516259  0.7008144021  11.656934697  1.3475557220  3400          0.0295795321 
0.6475251768  0.6498071153  0.6536668988  0.6440205744  0.6632024428  0.6525932276  11.999785718  1.3313663435  3500          0.0297857046 
0.6183308335  0.6131590227  0.6312744415  0.6290184312  0.7171479081  0.7104586369  12.342636738  1.3240646231  3600          0.0293641043 
0.5797085922  0.5827261037  0.6003107087  0.5912987570  0.7331119087  0.7254607801  12.685487759  1.3386396408  3700          0.0293782020 
0.6085279623  0.5938705529  0.6188460920  0.6200171453  0.7195050088  0.7106729533  13.028338779  1.3201306832  3800          0.0289101982 
0.5745661024  0.5788684098  0.5985964536  0.5917273896  0.7455402582  0.7372481783  13.371189800  1.3219193757  3900          0.0286351562 
0.5923505464  0.5968709816  0.6153104409  0.6048006858  0.7459688220  0.7402486069  13.714040820  1.3353839791  4000          0.0298259759 
0.6181165631  0.6165880840  0.6328279852  0.6183026147  0.7033802968  0.6939562795  14.056891841  1.3276646626  4100          0.0297114277 
0.6094921791  0.6043720532  0.6242567097  0.6193741963  0.7429688756  0.7359622803  14.399742861  1.3392763126  4200          0.0291427541 
0.5289265052  0.5291470210  0.5500080356  0.5428632662  0.6960947126  0.6888126875  14.742593882  1.3325644624  4300          0.0292419553 
0.6287229484  0.6290184312  0.6397921466  0.6358765538  0.7354690095  0.7310330047  15.085444902  1.3262486863  4400          0.0279905629 
0.5588172273  0.5600085727  0.5862752451  0.5745820832  0.7708790914  0.7623231890  15.428295923  1.3124546230  4500          0.0266580558 
0.6325798157  0.6303043292  0.6469170193  0.6363051865  0.7234692238  0.7160308616  15.771146943  1.2858451200  4600          0.0273973846 
0.6161345618  0.6170167167  0.6297208978  0.6281611659  0.7477902180  0.7383197600  16.113997964  1.3070253313  4700          0.0289755416 
0.6313477609  0.6324474925  0.6484169926  0.6365195028  0.7446295602  0.7372481783  16.456848984  1.2833492267  4800          0.0292761469 
0.6302764088  0.6326618088  0.6487384154  0.6352336048  0.7354690095  0.7254607801  16.799700005  1.3044953322  4900          0.0295065212 
0.6579708592  0.6525932276  0.6677559329  0.6639519931  0.7088444849  0.6980282898  17.142551025  1.2917007959  5000          0.0299710655 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/a5b94b7db84be19ccb9dc80f9e00d17c
	save_model_every_checkpoint: False
	seed: 1833283494
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          nll           penalty       step          step_time    
0.4221662738  0.4264894985  0.4473670113  0.4301328761  0.5801146408  0.5803686241  0.0000000000  0.7118185163  0.7041174769  0.0077010202  0             0.2740015984 
0.8554746090  0.8559794256  0.7647720576  0.7666095156  0.1660684631  0.1763823403  0.3428510205  0.7283904135  0.7077136487  0.0206767634  100           0.0259491134 
0.7769445040  0.7698242606  0.7114694380  0.7061723103  0.2902983875  0.2951135877  0.6857020410  0.6936402208  0.6935835516  0.0000566684  200           0.0249422383 
0.6607028069  0.6618088298  0.6197567901  0.6260180026  0.4187068088  0.4256322332  1.0285530616  0.6934252203  0.6933651531  0.0000600639  300           0.0271867228 
0.4571459181  0.4721388770  0.4862056035  0.4796399486  0.6338993947  0.6343763395  1.3714040821  0.6901713777  0.6896573907  0.0005139854  400           0.0270026112 
0.3323869724  0.3418345478  0.3933144051  0.3883411916  0.7999678577  0.8056150879  1.7142551026  0.6816022915  0.6810791481  0.0010445327  500           0.0271487641 
0.5131776302  0.5015002143  0.5057052553  0.5160737248  0.4993303691  0.5042863266  2.0571061231  0.7321691108  0.6943918145  0.0003777730  600           0.0254147458 
0.4910006428  0.5034290613  0.4978303959  0.4884269181  0.4982589597  0.4942134591  2.3999571436  0.6931470257  0.6929514343  0.0000019559  700           0.0262428355 
0.4910006428  0.5034290613  0.4978303959  0.4884269181  0.4982589597  0.4942134591  2.7428081641  0.6921738064  0.6906085300  0.0000156528  800           0.0250461078 
0.4928219413  0.4736390913  0.4944554562  0.4976425204  0.5370975518  0.5394342049  3.0856591847  0.6958758050  0.6925848991  0.0000329091  900           0.0253488445 
0.5089993572  0.4965709387  0.5021696041  0.5115730819  0.5017410403  0.5057865409  3.4285102052  0.6952011764  0.6940655673  0.0000113561  1000          0.0287623143 
0.4910006428  0.5034290613  0.4978303959  0.4884269181  0.4982589597  0.4942134591  3.7713612257  0.6935753024  0.6933524346  0.0000022287  1100          0.0285812807 
0.5090529248  0.4965709387  0.5022767451  0.5115730819  0.5017410403  0.5057865409  4.1142122462  0.6985998452  0.6942378885  0.0000436196  1200          0.0276445341 
0.6782729805  0.6817402486  0.6410242674  0.6491641663  0.4327958429  0.4382768967  4.4570632667  0.6967443323  0.6942281234  0.0000251621  1300          0.0282586551 
0.6548103707  0.6545220746  0.6205067767  0.6273039006  0.4417421117  0.4515645092  4.7999142872  0.6927055150  0.6904740632  0.0000223145  1400          0.0263352299 
0.6063852582  0.6065152165  0.5820431778  0.5927989713  0.4685809182  0.4755679383  5.1427653078  0.6918821871  0.6898696065  0.0000201258  1500          0.0263209891 
0.4910006428  0.5034290613  0.4978303959  0.4884269181  0.4982589597  0.4942134591  5.4856163283  0.6953246772  0.6922813869  0.0000304329  1600          0.0262519979 
0.3214591815  0.3375482212  0.3859752504  0.3795542220  0.8004499920  0.7991855979  5.8284673488  0.6929450285  0.6883080268  0.0000463700  1700          0.0258735561 
0.6450074995  0.6453064724  0.6149354476  0.6294470639  0.4522954947  0.4562794685  6.1713183693  0.6917007834  0.6871939623  0.0000450682  1800          0.0260989904 
0.3622776945  0.3780540077  0.4123319226  0.4166309473  0.7622542455  0.7689669953  6.5141693898  0.6938673854  0.6896685112  0.0000419887  1900          0.0268551898 
0.3308335119  0.3446206601  0.3959929287  0.3921988856  0.7851824075  0.7869695671  6.8570204103  0.6939894956  0.6890977293  0.0000489177  2000          0.0275909543 
0.4680737090  0.4770681526  0.4881877109  0.4751393056  0.5721862110  0.5762966138  7.1998714309  0.6940715039  0.6902192062  0.0000385230  2100          0.0275340557 
0.4853224770  0.4980711530  0.4952590132  0.4869267038  0.5156157926  0.5109301329  7.5427224514  0.6919083989  0.6879601151  0.0000394829  2200          0.0280003262 
0.4536640240  0.4601371625  0.4782771736  0.4699957137  0.5951679434  0.5955850836  7.8855734719  0.7013009161  0.6905022573  0.0001079866  2300          0.0281852174 
0.4717698736  0.4817831119  0.4915090802  0.4811401629  0.5667220228  0.5694384912  8.2284244924  0.6942649978  0.6860209548  0.0000824404  2400          0.0279203391 
0.3354938933  0.3508358337  0.3977071838  0.3919845692  0.7915037231  0.7953279040  8.5712755129  0.6916027892  0.6869998884  0.0000460290  2500          0.0282986069 
0.6592029141  0.6590227175  0.6276316494  0.6328761252  0.4558311459  0.4644234891  8.9141265335  0.6913579905  0.6844837165  0.0000687428  2600          0.0270320892 
0.6369723591  0.6356622375  0.6094712594  0.6204457780  0.4650452670  0.4714959280  9.2569775540  0.6930705005  0.6859210086  0.0000714949  2700          0.0279119921 
0.7021641311  0.7025289327  0.6619167515  0.6675953708  0.4354743665  0.4380625804  9.5998285745  0.6912315655  0.6869987404  0.0000423282  2800          0.0274258399 
0.4349153632  0.4442777540  0.4727058445  0.4657093871  0.6618096105  0.6630947278  9.9426795950  0.6913152564  0.6875653714  0.0000374988  2900          0.0278235841 
0.3354938933  0.3512644664  0.3985107409  0.3941277325  0.7943965286  0.7942563223  10.285530615  0.6927884704  0.6867521936  0.0000603628  3000          0.0277554321 
0.3219948575  0.3358336905  0.3873145122  0.3823403343  0.8175389725  0.8174024861  10.628381636  0.6926534760  0.6850786537  0.0000757482  3100          0.0276679635 
0.6039747161  0.5994427775  0.5785610971  0.5915130733  0.4750093748  0.4845692242  10.971232656  0.6900723118  0.6867237395  0.0000334857  3200          0.0280121922 
0.4520034283  0.4605657951  0.4838485027  0.4723531933  0.6311673006  0.6315902272  11.314083677  0.6957629102  0.6847097713  0.0001105314  3300          0.0271751785 
0.4482001286  0.4526360909  0.4777950394  0.4704243463  0.6323458510  0.6367338191  11.656934697  0.6890309268  0.6846199411  0.0000441098  3400          0.0270772195 
0.6219734305  0.6221603086  0.6000964268  0.6116588084  0.4686880591  0.4740677240  11.999785718  0.6959352374  0.6837573075  0.0001217793  3500          0.0271516395 
0.6980929934  0.6988855551  0.6605239192  0.6682383198  0.4412064070  0.4464209173  12.342636738  0.6918277699  0.6880353117  0.0000379246  3600          0.0273133039 
0.6524533962  0.6472353193  0.6231853003  0.6330904415  0.4581346762  0.4667809687  12.685487759  0.6905591470  0.6834536040  0.0000710554  3700          0.0273437333 
0.6920934219  0.6954564938  0.6559704291  0.6609515645  0.4386885948  0.4432061723  13.028338779  0.6888092864  0.6844205821  0.0000438871  3800          0.0280049801 
0.6663274052  0.6675953708  0.6330422671  0.6380197171  0.4529383404  0.4562794685  13.371189800  0.6935116929  0.6845068341  0.0000900486  3900          0.0274408913 
0.3554210414  0.3694813545  0.4132961911  0.4084869267  0.7780575347  0.7798971282  13.714040820  0.6919171405  0.6845697862  0.0000734735  4000          0.0274895811 
0.7493571888  0.7513930562  0.6975411153  0.7018859837  0.3952429421  0.3999142735  14.056891841  0.6892718858  0.6814087862  0.0000786310  4100          0.0277218580 
0.7093421898  0.7104586369  0.6701666042  0.6768109730  0.4328494134  0.4397771110  14.399742861  0.6954811853  0.6832631558  0.0001221803  4200          0.0276408648 
0.6738804371  0.6727389627  0.6386135962  0.6513073296  0.4509026625  0.4586369481  14.742593882  0.6916822934  0.6832661599  0.0000841613  4300          0.0276666760 
0.7480179987  0.7528932705  0.6987196657  0.7021003000  0.3962072106  0.4022717531  15.085444902  0.6863206875  0.6817119259  0.0000460876  4400          0.0278504515 
0.4370044997  0.4382768967  0.4701880324  0.4607801114  0.6778807521  0.6780968710  15.428295923  0.6908163241  0.6810735703  0.0000974276  4500          0.0280038214 
0.4182558389  0.4232747535  0.4548668774  0.4517788255  0.6739165372  0.6778825547  15.771146943  0.6842567867  0.6824806905  0.0000177609  4600          0.0270333076 
0.6494000429  0.6517359623  0.6207210586  0.6324474925  0.4610810521  0.4667809687  16.113997964  0.6953318137  0.6809995544  0.0001433225  4700          0.0272589922 
0.7326440969  0.7323189027  0.6871484438  0.6911701672  0.4217603257  0.4260608658  16.456848984  0.6859995985  0.6793383044  0.0000666130  4800          0.0276428628 
0.3841332762  0.3945563652  0.4349922323  0.4299185598  0.7278620025  0.7295327904  16.799700005  0.6926066017  0.6778780073  0.0001472859  4900          0.0275187492 
0.3484572531  0.3557651093  0.4044570633  0.3975567938  0.7638077891  0.7659665667  17.142551025  0.6940340519  0.6794376111  0.0001459644  5000          0.0276559854 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/cb8e9c13d29954c4c0453fda1be86270
	save_model_every_checkpoint: False
	seed: 429588860
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.1577030212  0.1562366052  0.2413349762  0.2370338620  0.8676809343  0.8626232319  0.0000000000  0.6987230182  0             0.2206630707 
0.4477180201  0.4494213459  0.4670273745  0.4792113159  0.5990785879  0.5906558080  0.3428510205  0.7094852942  100           0.0188178992 
0.4944825370  0.4837119589  0.5064552419  0.5068581226  0.6742379600  0.6697385341  0.6857020410  0.6890322357  200           0.0193983030 
0.6783801157  0.6718816974  0.6719880002  0.6765966567  0.7005946322  0.6855979426  1.0285530616  0.6471426648  300           0.0189484620 
0.7278765802  0.7209601372  0.7067016660  0.7104586369  0.6532919055  0.6476639520  1.3714040821  0.6061746174  400           0.0189855242 
0.7298050139  0.7263180454  0.7154872234  0.7203171882  0.6893984036  0.6757393913  1.7142551026  0.5957434458  500           0.0189708710 
0.7382151275  0.7408915559  0.7258263245  0.7291041577  0.7232549419  0.7117445349  2.0571061231  0.5825160867  600           0.0187664413 
0.7292157703  0.7312473210  0.7187014518  0.7269609944  0.7311298013  0.7209601372  2.3999571436  0.5807614410  700           0.0187839246 
0.7306620956  0.7325332190  0.7202014250  0.7250321474  0.7309690898  0.7153879126  2.7428081641  0.5793366379  800           0.0187031627 
0.7356438826  0.7331761680  0.7210049821  0.7248178311  0.7241656399  0.7096013716  3.0856591847  0.5797735769  900           0.0188422275 
0.7427148061  0.7438919846  0.7295226871  0.7336048007  0.7298441099  0.7149592799  3.4285102052  0.5834347498  1000          0.0182768607 
0.7408399400  0.7417488213  0.7268977340  0.7314616374  0.7323083516  0.7149592799  3.7713612257  0.5765148479  1100          0.0186490464 
0.7200021427  0.7186026575  0.7103980286  0.7151735962  0.7435045803  0.7256750964  4.1142122462  0.5861271417  1200          0.0188616705 
0.7380544247  0.7346763823  0.7217013982  0.7233176168  0.7267370226  0.7096013716  4.4570632667  0.5694864881  1300          0.0187415743 
0.7263231198  0.7271753108  0.7140943912  0.7141020146  0.7325762040  0.7115302186  4.7999142872  0.5703596774  1400          0.0186783195 
0.7424469681  0.7408915559  0.7260941769  0.7297471067  0.7368082713  0.7252464638  5.1427653078  0.5820616487  1500          0.0185711789 
0.7375723163  0.7372481783  0.7237906466  0.7273896271  0.7377189693  0.7171024432  5.4856163283  0.5689053822  1600          0.0185419250 
0.7361259910  0.7288898414  0.7189157336  0.7267466781  0.7353618685  0.7175310759  5.8284673488  0.5701034921  1700          0.0181655526 
0.7441611313  0.7406772396  0.7275405796  0.7340334333  0.7374511169  0.7196742392  6.1713183693  0.5673576590  1800          0.0177661443 
0.7410006428  0.7333904844  0.7230406600  0.7243891985  0.7424867413  0.7235319331  6.5141693898  0.5604836902  1900          0.0181936145 
0.7412149132  0.7351050150  0.7251834789  0.7301757394  0.7440938555  0.7248178311  6.8570204103  0.5737070230  2000          0.0184353900 
0.7421791301  0.7351050150  0.7245406332  0.7258894128  0.7478973590  0.7239605658  7.1998714309  0.5596302450  2100          0.0184821033 
0.7495178916  0.7426060866  0.7279155729  0.7331761680  0.7421117480  0.7228889841  7.5427224514  0.5545785230  2200          0.0183211613 
0.7562138419  0.7471067295  0.7310226603  0.7368195456  0.7322547812  0.7104586369  7.8855734719  0.5618415985  2300          0.0181482601 
0.7475358903  0.7344620660  0.7212192639  0.7267466781  0.7482187818  0.7254607801  8.2284244924  0.5538697016  2400          0.0175043774 
0.7200557103  0.7038148307  0.6951840146  0.7059579940  0.7482723523  0.7181740249  8.5712755129  0.5524337280  2500          0.0181707072 
0.7365009642  0.7117445349  0.7094337601  0.7156022289  0.7562543526  0.7226746678  8.9141265335  0.5570497963  2600          0.0181477475 
0.7504821084  0.7348906987  0.7195050088  0.7233176168  0.7441474259  0.7117445349  9.2569775540  0.5458777687  2700          0.0181381083 
0.7435183201  0.7237462495  0.7132908341  0.7196742392  0.7529329833  0.7181740249  9.5998285745  0.5418467450  2800          0.0184542322 
0.7513391901  0.7288898414  0.7162907805  0.7201028718  0.7592542990  0.7226746678  9.9426795950  0.5488342881  2900          0.0177544498 
0.7550889222  0.7310330047  0.7187550222  0.7252464638  0.7532544062  0.7158165452  10.285530615  0.5352305010  3000          0.0172504377 
0.7586779516  0.7207458208  0.7125408475  0.7224603515  0.7652006214  0.7213887698  10.628381636  0.5329481429  3100          0.0170787406 
0.7645703878  0.7271753108  0.7175764718  0.7205315045  0.7578078963  0.7100300043  10.971232656  0.5305350429  3200          0.0174584103 
0.7733019070  0.7408915559  0.7223442439  0.7312473210  0.7527722719  0.7070295757  11.314083677  0.5287240666  3300          0.0175212193 
0.7598564388  0.7063866267  0.6965232764  0.6986712387  0.7719505009  0.7081011573  11.656934697  0.5189091188  3400          0.0178362322 
0.7600171416  0.7098156880  0.7033267263  0.7111015859  0.7796646488  0.7213887698  11.999785718  0.5188182473  3500          0.0180876732 
0.7651060639  0.7132447492  0.7019338940  0.7128161166  0.7814324744  0.7207458208  12.342636738  0.4963228714  3600          0.0183703089 
0.7729269338  0.7046720960  0.6901483902  0.7014573511  0.7894680452  0.7130304329  12.685487759  0.4937761593  3700          0.0185001731 
0.7840154275  0.7040291470  0.6935233299  0.6948135448  0.7920394279  0.7038148307  13.028338779  0.4918812892  3800          0.0184593987 
0.7959610028  0.7025289327  0.6979696791  0.7042434634  0.7898966090  0.6984569224  13.371189800  0.4770735219  3900          0.0184023714 
0.7869616456  0.6892413202  0.6840949269  0.6898842692  0.8085391332  0.7166738105  13.714040820  0.4622060904  4000          0.0184116411 
0.8017998714  0.6995285041  0.6863448867  0.6980282898  0.7992714416  0.6849549936  14.056891841  0.4589411792  4100          0.0184846330 
0.7988000857  0.6975996571  0.6850591954  0.6948135448  0.8170568383  0.7089584226  14.399742861  0.4529154184  4200          0.0183391023 
0.8044782516  0.6780968710  0.6685059195  0.6804543506  0.8089141265  0.6868838405  14.742593882  0.4357210651  4300          0.0185330963 
0.8321191343  0.7068152593  0.6910055178  0.7016716674  0.8053784754  0.6635233605  15.085444902  0.4092648670  4400          0.0185649896 
0.8459931433  0.6930990141  0.6791664435  0.6821688813  0.8256816843  0.6613801972  15.428295923  0.4118159720  4500          0.0186017752 
0.8479215770  0.6847406772  0.6678630739  0.6772396057  0.8344672417  0.6673810544  15.771146943  0.3949239722  4600          0.0185914183 
0.8494214699  0.6813116159  0.6686666310  0.6725246464  0.8434670809  0.6609515645  16.113997964  0.3685011676  4700          0.0183985257 
0.8576173130  0.6729532790  0.6652381207  0.6789541363  0.8486634167  0.6543077583  16.456848984  0.3710271680  4800          0.0185316634 
0.8598135847  0.6881697385  0.6734344030  0.6748821260  0.8493062624  0.6622374625  16.799700005  0.3534967253  4900          0.0185495687 
0.8672059139  0.6804543506  0.6700058928  0.6733819117  0.8606632024  0.6643806258  17.142551025  0.3282093120  5000          0.0180705953 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/03598bac7f7f94d9e026ad5ee7e6c8cf
	save_model_every_checkpoint: False
	seed: 1563456041
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.6973430469  0.6935276468  0.6471848717  0.6592370339  0.3057266835  0.3133304758  0.0000000000  0.7356665730  0             0.3215305805 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  0.3428510205  0.4444691399  100           0.0185619497 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  0.6857020410  0.4391658568  200           0.0186454773 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  1.0285530616  0.4290122992  300           0.0185761714 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  1.3714040821  0.4267162690  400           0.0186696625 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  1.7142551026  0.4159787789  500           0.0183986235 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  2.0571061231  0.4080476451  600           0.0183930111 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  2.3999571436  0.4043958679  700           0.0188259196 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  2.7428081641  0.4016515584  800           0.0184436822 
0.8938825798  0.8941277325  0.7928429849  0.8126875268  0.1146408100  0.1215173596  3.0856591847  0.3920053199  900           0.0186026955 
0.8986500964  0.8986283755  0.7942893877  0.8124732105  0.0987839503  0.1065152165  3.4285102052  0.3890727231  1000          0.0180445552 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  3.7713612257  0.3840647700  1100          0.0178542590 
0.8985965288  0.8988426918  0.7942893877  0.8126875268  0.0981946751  0.1063009001  4.1142122462  0.3721937531  1200          0.0175225949 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  4.4570632667  0.3708123446  1300          0.0183038831 
0.8986500964  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  4.7999142872  0.3752601209  1400          0.0170966363 
0.8985965288  0.8992713245  0.7940751058  0.8126875268  0.0992660845  0.1067295328  5.1427653078  0.3768437865  1500          0.0180830908 
0.8962395543  0.8941277325  0.7932715487  0.8107586798  0.1092837628  0.1185169310  5.4856163283  0.3654286614  1600          0.0175690722 
0.8987036640  0.8986283755  0.7941286763  0.8126875268  0.0984625275  0.1060865838  5.8284673488  0.3747751915  1700          0.0165258479 
0.8987036640  0.8984140592  0.7943429582  0.8126875268  0.0988375208  0.1067295328  6.1713183693  0.3635988668  1800          0.0169601798 
0.8966680951  0.8951993142  0.7940215353  0.8099014145  0.1113194407  0.1176596657  6.5141693898  0.3727499649  1900          0.0170503974 
0.8987036640  0.8988426918  0.7942893877  0.8126875268  0.0981411046  0.1060865838  6.8570204103  0.3703230594  2000          0.0172771072 
0.8973109064  0.8964852122  0.7950929448  0.8120445778  0.1068195211  0.1172310330  7.1998714309  0.3740029868  2100          0.0178233814 
0.8959717163  0.8945563652  0.7946108105  0.8109729961  0.1097123266  0.1185169310  7.5427224514  0.3665379959  2200          0.0176765037 
0.8985965288  0.8986283755  0.7943965286  0.8129018431  0.0984089570  0.1063009001  7.8855734719  0.3615756729  2300          0.0174093771 
0.8981144204  0.8981997428  0.7943965286  0.8126875268  0.1017303262  0.1095156451  8.2284244924  0.3594252321  2400          0.0175106335 
0.8981144204  0.8969138448  0.7947715219  0.8122588941  0.1033910109  0.1099442778  8.5712755129  0.3738942170  2500          0.0170894814 
0.8986500964  0.8988426918  0.7942893877  0.8129018431  0.0981411046  0.1060865838  8.9141265335  0.3624636565  2600          0.0175947285 
0.8885258196  0.8861980283  0.7962179247  0.8030432919  0.1531044088  0.1654522075  9.2569775540  0.3631297678  2700          0.0173496389 
0.8890079280  0.8872696099  0.7964322066  0.8045435062  0.1589435903  0.1581654522  9.5998285745  0.3618795308  2800          0.0182408023 
0.8957574459  0.8932704672  0.7970214818  0.8088298328  0.1240692130  0.1315902272  9.9426795950  0.3584297925  2900          0.0174498916 
0.8976323120  0.8949849979  0.7971821932  0.8105443635  0.1164086356  0.1230175739  10.285530615  0.3658239573  3000          0.0192450500 
0.8988643668  0.8986283755  0.7943965286  0.8131161595  0.0992125141  0.1080154308  10.628381636  0.3640906924  3100          0.0189778352 
0.8988643668  0.8990570081  0.7944500991  0.8122588941  0.1004446349  0.1082297471  10.971232656  0.3645228301  3200          0.0195736885 
0.8989179344  0.8977711102  0.7962179247  0.8120445778  0.1057481116  0.1135876554  11.314083677  0.3578388880  3300          0.0193163824 
0.8928112278  0.8849121303  0.7994321530  0.8073296185  0.1531579793  0.1577368195  11.656934697  0.3551436290  3400          0.0191227698 
0.8960788515  0.8872696099  0.8034499384  0.8071153022  0.1505330262  0.1525932276  11.999785718  0.3575216211  3500          0.0188645697 
0.8974716092  0.8902700386  0.8012535490  0.8086155165  0.1361761397  0.1431633090  12.342636738  0.3470724833  3600          0.0185752249 
0.8946325262  0.8793399057  0.8100391064  0.8051864552  0.1806396314  0.1868838405  12.685487759  0.3439504510  3700          0.0192039442 
0.8958645811  0.8825546507  0.8052177640  0.8051864552  0.1574971876  0.1630947278  13.028338779  0.3382692851  3800          0.0191584516 
0.8960788515  0.8836262323  0.8105748112  0.8054007715  0.1793003696  0.1845263609  13.371189800  0.3526069811  3900          0.0191006446 
0.8994536105  0.8853407630  0.8095034017  0.8062580369  0.1537472545  0.1626660952  13.714040820  0.3388610902  4000          0.0189041018 
0.8980072852  0.8846978140  0.8119676434  0.8064723532  0.1659077516  0.1753107587  14.056891841  0.3282307716  4100          0.0176978111 
0.8958110135  0.8778396914  0.8159854288  0.8073296185  0.1868002357  0.1898842692  14.399742861  0.3343869492  4200          0.0181867123 
0.8943646882  0.8724817831  0.8235924358  0.7970424346  0.2325494188  0.2355336477  14.742593882  0.3245381956  4300          0.0193079424 
0.8945253910  0.8591941706  0.8350029464  0.7923274754  0.2554775808  0.2563223318  15.085444902  0.3184752148  4400          0.0183665848 
0.8996143133  0.8774110587  0.8217174693  0.8024003429  0.1961214978  0.2046720960  15.428295923  0.3171432927  4500          0.0194772863 
0.9018641526  0.8596228033  0.8432527991  0.7972567510  0.2605132051  0.2644663523  15.771146943  0.3129981244  4600          0.0196439910 
0.9069530748  0.8673381912  0.8405207050  0.8009001286  0.2279959286  0.2265323618  16.113997964  0.3079448053  4700          0.0197979093 
0.9109706450  0.8690527218  0.8443777790  0.8015430776  0.2162639953  0.2203171882  16.456848984  0.3025069581  4800          0.0193317437 
0.9125776730  0.8671238748  0.8502169604  0.7978997000  0.2178175390  0.2241748821  16.799700005  0.3070143622  4900          0.0172865081 
0.9078101564  0.8709815688  0.8435742219  0.8034719246  0.2094605453  0.2183883412  17.142551025  0.2816407518  5000          0.0176262331 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/95e4e4bf68d698f896fbf9ea740ef75b
	save_model_every_checkpoint: False
	seed: 1579483688
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0, 1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          nll           penalty       step          step_time    
0.4367366617  0.4560651522  0.4536883270  0.4554222032  0.5718647881  0.5619374196  0.0000000000  0.7235487103  0.7223417163  0.0012070197  0             0.2401816845 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  0.3428510205  0.3703719515  0.3529471293  0.0174248214  100           0.0130519319 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  0.6857020410  0.3383322094  0.3373468517  0.0009853576  200           0.0131658077 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  1.0285530616  0.3391498746  0.3396160561  -0.000466181  300           0.0134667206 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  1.3714040821  0.3236877923  0.3249542342  -0.001266442  400           0.0131167054 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  1.7142551026  0.3147162709  0.3135554233  -0.000270647  500           0.0128122568 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  2.0571061231  0.5412098171  0.3258134744  0.0021539633  600           0.0144626880 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  2.3999571436  0.3994995321  0.3288652349  0.0007063429  700           0.0137541533 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  2.7428081641  0.3903043027  0.3204142722  0.0006989003  800           0.0126636600 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  3.0856591847  0.0075738709  0.3410924841  -0.003335186  900           0.0126861072 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  3.4285102052  0.2432221210  0.3726491264  -0.001294270  1000          0.0129349613 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  3.7713612257  0.0879280455  0.3291893400  -0.002412612  1100          0.0118570900 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  4.1142122462  0.3167423053  0.3849905466  -0.000682482  1200          0.0122700834 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  4.4570632667  0.6096205263  0.3920060447  0.0021761448  1300          0.0134741044 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  4.7999142872  0.4985477734  0.3712218949  0.0012732587  1400          0.0136391068 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  5.1427653078  0.4771889110  0.3483299103  0.0012885900  1500          0.0145986724 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  5.4856163283  0.2836594243  0.3570307122  -0.000733712  1600          0.0130156803 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  5.8284673488  0.5656932367  0.3627350363  0.0020295820  1700          0.0118157792 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  6.1713183693  0.0156891830  0.3552761599  -0.003395869  1800          0.0118856907 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  6.5141693898  0.6689105590  0.3646874759  0.0030422308  1900          0.0115377760 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  6.8570204103  0.5495597023  0.3667675726  0.0018279213  2000          0.0123869419 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  7.1998714309  0.5618989198  0.4042832759  0.0015761564  2100          0.0122277737 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  7.5427224514  0.5209295651  0.3933841179  0.0012754545  2200          0.0110721779 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  7.8855734719  0.5520889008  0.3626838264  0.0018940508  2300          0.0116283751 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  8.2284244924  0.3204643458  0.3331953938  -0.000127310  2400          0.0122559476 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  8.5712755129  0.1102685651  0.3354074936  -0.002251389  2500          0.0111067009 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  8.9141265335  0.2697896710  0.3537970433  -0.000840073  2600          0.0120603371 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  9.2569775540  0.3466378345  0.3536978568  -0.000070600  2700          0.0113164639 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  9.5998285745  0.2634376270  0.3257345225  -0.000622969  2800          0.0116262984 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  9.9426795950  0.5464728844  0.3216100695  0.0022486281  2900          0.0116549897 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  10.285530615  0.3747479293  0.3425889748  0.0003215895  3000          0.0124434185 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  10.628381636  0.3904843967  0.3427698645  0.0004771454  3100          0.0126202631 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  10.971232656  0.5222677457  0.3350927241  0.0018717502  3200          0.0131695843 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  11.314083677  0.4570209551  0.3374749096  0.0011954605  3300          0.0140940189 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  11.656934697  0.4205910805  0.3471515025  0.0007343957  3400          0.0128844142 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  11.999785718  0.3316303669  0.3486085778  -0.000169782  3500          0.0135959458 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  12.342636738  0.3151060402  0.3391416311  -0.000240355  3600          0.0135026622 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  12.685487759  0.3937060431  0.3495547552  0.0004415128  3700          0.0117475939 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  13.028338779  0.3053129978  0.3396647407  -0.000343517  3800          0.0124241710 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  13.371189800  0.4777178265  0.3297745395  0.0014794329  3900          0.0113416624 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  13.714040820  0.4086804463  0.3333577818  0.0007532266  4000          0.0109579134 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  14.056891841  0.5890740030  0.3489476787  0.0024012633  4100          0.0116591287 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  14.399742861  0.5464367419  0.3177329819  0.0022870376  4200          0.0123125911 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  14.742593882  0.4736314790  0.3336712356  0.0013996025  4300          0.0129537439 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  15.085444902  0.1057698914  0.3463504428  -0.002405805  4400          0.0119709659 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  15.428295923  0.1974029566  0.3390216753  -0.001416187  4500          0.0123284197 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  15.771146943  0.1713141721  0.3415926783  -0.001702785  4600          0.0121490479 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  16.113997964  0.3375725828  0.3325137007  0.0000505887  4700          0.0123187900 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  16.456848984  0.3803792521  0.3318560418  0.0004852322  4800          0.0138758564 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  16.799700005  0.3246825987  0.3271905449  -0.000025079  4900          0.0118689060 
0.1015641740  0.1015859408  0.1965500616  0.2104586369  0.9008410564  0.8934847835  17.142551025  0.3333478242  0.3287883604  0.0000455946  5000          0.0110445166 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: AnticausalReg
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/3bbd0e3f41e1464551088bbfc341769c
	save_model_every_checkpoint: False
	seed: 733028084
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.4644311121  0.4774967853  0.4762950662  0.4727818260  0.5179193229  0.5062151736  0.0000000000  418.78082275  0             0.2322981358 
0.6680951361  0.6648092585  0.6277923609  0.6369481354  0.3154765093  0.3114016288  0.3428510205  17.446417163  100           0.0156372476 
0.5132311978  0.5218602658  0.5125622757  0.5130732962  0.4741522473  0.4635662237  0.6857020410  1.5849589813  200           0.0144110537 
0.5679772873  0.5672953279  0.5492044785  0.5535790827  0.3959393582  0.3926275182  1.0285530616  1.4995976937  300           0.0133993387 
0.7732483394  0.7651093013  0.6994160819  0.7072438920  0.1816574704  0.1843120446  1.3714040821  1.4695273960  400           0.0123112965 
0.8932397686  0.8816973853  0.7940751058  0.8051864552  0.1081052124  0.1110158594  1.7142551026  1.4453619635  500           0.0134427023 
0.8912041997  0.8799828547  0.7928965554  0.8054007715  0.1079445010  0.1118731247  2.0571061231  1.4252298129  600           0.0143640447 
0.8668309406  0.8546935276  0.7792896555  0.7839691384  0.1724969197  0.1729532790  2.3999571436  1.4018239164  700           0.0151542807 
0.8990786372  0.8866266610  0.7994857235  0.8094727818  0.1063373868  0.1090870124  2.7428081641  1.3750188982  800           0.0148592257 
0.9018105850  0.8883411916  0.8008249853  0.8120445778  0.1019446081  0.1048006858  3.0856591847  1.3342855513  900           0.0145566773 
0.9019177202  0.8885555079  0.8010392672  0.8120445778  0.1018910377  0.1048006858  3.4285102052  1.2850998938  1000          0.0138367558 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  3.7713612257  1.2266006041  1100          0.0138861370 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  4.1142122462  1.1798087120  1200          0.0139892268 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  4.4570632667  1.1329242998  1300          0.0141747499 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  4.7999142872  1.0875578392  1400          0.0145631289 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  5.1427653078  1.0452984095  1500          0.0152993083 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  5.4856163283  1.0057269371  1600          0.0155189586 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  5.8284673488  0.9829978013  1700          0.0153062510 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  6.1713183693  0.9659052032  1800          0.0143445659 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  6.5141693898  0.9530319673  1900          0.0131483936 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  6.8570204103  0.9404221344  2000          0.0149428272 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  7.1998714309  0.9144827938  2100          0.0156654119 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  7.5427224514  0.8954120398  2200          0.0145812511 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  7.8855734719  0.8674728787  2300          0.0138378811 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  8.2284244924  0.8665234619  2400          0.0141205120 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  8.5712755129  0.8474887687  2500          0.0147612143 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  8.9141265335  0.8508863592  2600          0.0146358132 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  9.2569775540  0.8284757572  2700          0.0146740460 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  9.5998285745  0.8286403024  2800          0.0147691727 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  9.9426795950  0.8179867083  2900          0.0160343337 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  10.285530615  0.8011451244  3000          0.0158557653 
0.9019712878  0.8887698243  0.8011464081  0.8120445778  0.1017303262  0.1048006858  10.628381636  0.8132638991  3100          0.0145372462 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  10.971232656  0.8046251744  3200          0.0141731501 
0.9019712878  0.8883411916  0.8010392672  0.8120445778  0.1019446081  0.1050150021  11.314083677  0.7865528536  3300          0.0151946640 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015696148  0.1048006858  11.656934697  0.7710664323  3400          0.0136788249 
0.9020248554  0.8885555079  0.8010392672  0.8120445778  0.1017838967  0.1048006858  11.999785718  0.7644392234  3500          0.0141263747 
0.9016498822  0.8874839263  0.8010928376  0.8114016288  0.1037124337  0.1080154308  12.342636738  0.7926494753  3600          0.0150655580 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  12.685487759  0.7868799865  3700          0.0139780545 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015696148  0.1048006858  13.028338779  0.7802147818  3800          0.0132772660 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015696148  0.1048006858  13.371189800  0.7600478840  3900          0.0138221860 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  13.714040820  0.7517083988  4000          0.0128686166 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015696148  0.1048006858  14.056891841  0.7815414006  4100          0.0134844017 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1016767558  0.1048006858  14.399742861  0.7565482423  4200          0.0131654406 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1017303262  0.1052293185  14.742593882  0.7275013363  4300          0.0139635468 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  15.085444902  0.7310501841  4400          0.0141793466 
0.9020248554  0.8887698243  0.8010928376  0.8120445778  0.1016767558  0.1048006858  15.428295923  0.7413986200  4500          0.0142173862 
0.9019177202  0.8887698243  0.8010392672  0.8118302615  0.1022660310  0.1052293185  15.771146943  0.7204856035  4600          0.0146084118 
0.9020248554  0.8885555079  0.8011464081  0.8120445778  0.1019981786  0.1052293185  16.113997964  0.7303479972  4700          0.0143620062 
0.9020248554  0.8887698243  0.8011464081  0.8120445778  0.1015160444  0.1048006858  16.456848984  0.7516968992  4800          0.0139045119 
0.9020248554  0.8887698243  0.8010392672  0.8120445778  0.1019446081  0.1048006858  16.799700005  0.7166996187  4900          0.0131769824 
0.9020248554  0.8881268753  0.8010392672  0.8118302615  0.1025874538  0.1058722675  17.142551025  0.7271308029  5000          0.0128716254 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/3e5416cc23f84f8f7c1966e2a8160aee
	save_model_every_checkpoint: False
	seed: 1380669446
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1, 2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          nll           penalty       step          step_time    
0.4970002143  0.4867123875  0.5011517651  0.4920702958  0.4905448117  0.4871410201  0.0000000000  0.7596686482  0.7541636229  0.0055050193  0             0.2077479362 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  0.3428510205  0.3729725005  0.3506589004  0.0223135999  100           0.0122600508 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  0.6857020410  0.3232168251  0.3200116009  0.0032052230  200           0.0125344682 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  1.0285530616  0.3256016809  0.3231938232  0.0024078576  300           0.0132117724 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  1.3714040821  0.3351083335  0.3338563091  0.0012520242  400           0.0134810662 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  1.7142551026  0.3186391197  0.3293323476  0.0025103652  500           0.0136243939 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  2.0571061231  0.3077879138  0.3386690705  -0.000308811  600           0.0125552893 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  2.3999571436  0.4074301583  0.3458185455  0.0006161162  700           0.0128173566 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  2.7428081641  0.2789109546  0.3296984094  -0.000507874  800           0.0127207112 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  3.0856591847  0.4104025942  0.3295058888  0.0008089670  900           0.0131978321 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  3.4285102052  1.0242342669  0.3209184619  0.0070331579  1000          0.0125985408 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  3.7713612257  0.2551920560  0.3540671042  -0.000988750  1100          0.0119737029 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  4.1142122462  0.5214965235  0.3431018032  0.0017839473  1200          0.0121914005 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  4.4570632667  0.3282668616  0.3713404958  -0.000430736  1300          0.0116457772 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  4.7999142872  0.4690588307  0.3247399086  0.0014431893  1400          0.0120516157 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  5.1427653078  0.5303459199  0.3346254113  0.0019572052  1500          0.0115712953 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  5.4856163283  0.4559998704  0.3340296951  0.0012197017  1600          0.0117068934 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  5.8284673488  0.1629827695  0.3222536550  -0.001592708  1700          0.0123311567 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  6.1713183693  0.2945149872  0.3089685430  -0.000144535  1800          0.0123610711 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  6.5141693898  0.2293300201  0.3247946931  -0.000954646  1900          0.0117793965 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  6.8570204103  0.4209798872  0.3366968203  0.0008428306  2000          0.0123645139 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  7.1998714309  0.4924224488  0.3262782963  0.0016614414  2100          0.0123536444 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  7.5427224514  0.2380830584  0.3193697329  -0.000812866  2200          0.0122102499 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  7.8855734719  0.6417528482  0.3495924243  0.0029216043  2300          0.0124881673 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  8.2284244924  0.3506715064  0.3323560038  0.0001831550  2400          0.0122603559 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  8.5712755129  0.2348601496  0.3567245154  -0.001218643  2500          0.0122965789 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  8.9141265335  0.4306609069  0.3270349805  0.0010362593  2600          0.0129909992 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  9.2569775540  0.2680387169  0.3610068090  -0.000929680  2700          0.0114820242 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  9.5998285745  0.4398435313  0.3454417822  0.0009440175  2800          0.0123004913 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  9.9426795950  0.4606677672  0.3504388922  0.0011022887  2900          0.0120730662 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  10.285530615  0.2136854789  0.3578185296  -0.001441330  3000          0.0116566157 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  10.628381636  0.4105114679  0.3558808306  0.0005463064  3100          0.0119371223 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  10.971232656  0.2367279099  0.3258891456  -0.000891612  3200          0.0133282065 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  11.314083677  0.2976326439  0.3116286401  -0.000139959  3300          0.0127723169 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  11.656934697  0.1762710032  0.3249212347  -0.001486502  3400          0.0134313297 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  11.999785718  0.3011378904  0.3215134422  -0.000203755  3500          0.0137421250 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  12.342636738  0.1420949546  0.3238795488  -0.001817846  3600          0.0138225269 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  12.685487759  0.4984475711  0.3331553672  0.0016529220  3700          0.0125682616 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  13.028338779  0.5474759612  0.3195039676  0.0022797200  3800          0.0124665451 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  13.371189800  0.2568199883  0.3238049176  -0.000669849  3900          0.0125566101 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  13.714040820  0.5425748719  0.3357806797  0.0020679419  4000          0.0125636721 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  14.056891841  0.4052546808  0.3322327861  0.0007302190  4100          0.0126939368 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  14.399742861  0.2276125349  0.3135271254  -0.000859145  4200          0.0125723481 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  14.742593882  0.0239550681  0.3280939355  -0.003041388  4300          0.0125785756 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  15.085444902  0.1945609979  0.3147990263  -0.001202380  4400          0.0125528264 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  15.428295923  0.2764593460  0.3219352114  -0.000454758  4500          0.0124776912 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  15.771146943  0.4679284193  0.3399874803  0.0012794094  4600          0.0119454098 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  16.113997964  0.4448857360  0.3259188735  0.0011896686  4700          0.0124125266 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  16.456848984  0.2819961502  0.3536661974  -0.000716700  4800          0.0126701713 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  16.799700005  0.4092952028  0.3529013245  0.0005639388  4900          0.0132087064 
0.9030962074  0.8936990999  0.7945036696  0.8114016288  0.1019981786  0.0985855122  17.142551025  0.5824749160  0.3229830250  0.0025949189  5000          0.0138906360 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/40a82c64dfe9a2e552fd9bf6559eec08
	save_model_every_checkpoint: False
	seed: 219572418
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.4922862653  0.4914273468  0.4944554562  0.4888555508  0.4964375636  0.4984997857  0.0000000000  0.7346124053  0             0.3338406086 
0.1007606600  0.0981568796  0.2004071356  0.1986712387  0.8989125194  0.9054864981  0.3428510205  0.7025810689  100           0.0184540248 
0.1588279409  0.1590227175  0.2507633792  0.2430347192  0.8838592168  0.8921988856  0.6857020410  0.6827664119  200           0.0181899786 
0.5652989072  0.5615087870  0.5806503455  0.5741534505  0.6755236514  0.6746678097  1.0285530616  0.6618956852  300           0.0183058071 
0.6991107778  0.6950278611  0.7062731023  0.7048864123  0.7309690898  0.7355336477  1.3714040821  0.6141673642  400           0.0182342863 
0.6872723377  0.6753107587  0.6977553972  0.6954564938  0.7447367011  0.7453921989  1.7142551026  0.5926158231  500           0.0179239798 
0.6892543390  0.6815259323  0.7009160551  0.7006000857  0.7392725130  0.7419631376  2.0571061231  0.5878750396  600           0.0177568936 
0.7174844654  0.7175310759  0.7280227139  0.7207458208  0.7340226067  0.7353193313  2.3999571436  0.5871544924  700           0.0178679132 
0.7062888365  0.6986712387  0.7173621900  0.7083154736  0.7478437885  0.7453921989  2.7428081641  0.5814332345  800           0.0179592371 
0.6930576387  0.6907415345  0.7082016393  0.7042434634  0.7490223389  0.7505357908  3.0856591847  0.5746153036  900           0.0174098134 
0.6953610456  0.6888126875  0.7092194782  0.7048864123  0.7472545133  0.7486069438  3.4285102052  0.5740619427  1000          0.0177508378 
0.6817548747  0.6765966567  0.6938983232  0.6862408916  0.7142551026  0.7183883412  3.7713612257  0.5729607475  1100          0.0181129265 
0.7026462396  0.6982426061  0.7180586061  0.7074582083  0.7510580168  0.7503214745  4.1142122462  0.5764615649  1200          0.0172739387 
0.7096635955  0.7087441063  0.7267370226  0.7123874839  0.7435581507  0.7426060866  4.4570632667  0.5719734776  1300          0.0175921535 
0.7187700879  0.7173167595  0.7330047678  0.7220317188  0.7422724594  0.7400342906  4.7999142872  0.5642645690  1400          0.0173456502 
0.7062352689  0.7006000857  0.7237370761  0.7093870553  0.7500937483  0.7479639949  5.1427653078  0.5583227703  1500          0.0167959833 
0.7174844654  0.7156022289  0.7322547812  0.7209601372  0.7424867413  0.7351050150  5.4856163283  0.5708513555  1600          0.0168692732 
0.7109492179  0.7100300043  0.7310226603  0.7188169739  0.7480580704  0.7398199743  5.8284673488  0.5753073263  1700          0.0170141935 
0.7148596529  0.7132447492  0.7342368886  0.7183883412  0.7483259228  0.7477496785  6.1713183693  0.5585526836  1800          0.0168028212 
0.7150739233  0.7162451779  0.7363797075  0.7207458208  0.7477366476  0.7464637805  6.5141693898  0.5632500798  1900          0.0167456007 
0.7170023570  0.7190312902  0.7378796807  0.7222460351  0.7471473724  0.7398199743  6.8570204103  0.5597167239  2000          0.0167285943 
0.7068780801  0.7070295757  0.7311833717  0.7128161166  0.7545936680  0.7460351479  7.1998714309  0.5576448953  2100          0.0172233915 
0.7076815942  0.7072438920  0.7345583115  0.7149592799  0.7536293995  0.7505357908  7.5427224514  0.5499014685  2200          0.0169247937 
0.7085386758  0.7057436777  0.7317190764  0.7130304329  0.7516472920  0.7419631376  7.8855734719  0.5590064058  2300          0.0170494533 
0.7078958646  0.7046720960  0.7380403921  0.7136733819  0.7577007553  0.7473210459  8.2284244924  0.5624192396  2400          0.0168653512 
0.6974501821  0.6937419631  0.7278620025  0.7008144021  0.7634863663  0.7563223318  8.5712755129  0.5522609273  2500          0.0169995999 
0.7023248339  0.7001714531  0.7325762040  0.7102443206  0.7610756951  0.7471067295  8.9141265335  0.5583715996  2600          0.0168532610 
0.7077351618  0.7055293613  0.7400224996  0.7153879126  0.7607007018  0.7417488213  9.2569775540  0.5533935484  2700          0.0173701835 
0.6862009856  0.6849549936  0.7259870359  0.6930990141  0.7709326619  0.7539648521  9.5998285745  0.5512715128  2800          0.0178286076 
0.7045746732  0.7031718817  0.7404510634  0.7106729533  0.7530401243  0.7333904844  9.9426795950  0.5397745517  2900          0.0172957134 
0.7129847868  0.7147449636  0.7484330637  0.7151735962  0.7553436546  0.7301757394  10.285530615  0.5347165838  3000          0.0176409912 
0.6781122777  0.6759537077  0.7281298548  0.6898842692  0.7778432528  0.7554650664  10.628381636  0.5383069289  3100          0.0169895983 
0.6936468824  0.6898842692  0.7453795468  0.6954564938  0.7768789843  0.7490355765  10.971232656  0.5343542367  3200          0.0172602463 
0.6833083351  0.6830261466  0.7410939090  0.6943849121  0.7761825682  0.7447492499  11.314083677  0.5309073153  3300          0.0174027419 
0.6832011999  0.6810972996  0.7448438421  0.6907415345  0.7794503670  0.7357479640  11.656934697  0.5321280399  3400          0.0170870137 
0.6768802228  0.6699528504  0.7407189157  0.6918131162  0.7809503402  0.7353193313  11.999785718  0.5131504405  3500          0.0176134086 
0.6738268695  0.6635233605  0.7455938287  0.6860265752  0.7940751058  0.7458208315  12.342636738  0.5166364187  3600          0.0174697018 
0.6943968288  0.6930990141  0.7639685006  0.6935276468  0.7787539508  0.7224603515  12.685487759  0.5147879517  3700          0.0174551082 
0.6822369831  0.6813116159  0.7627899502  0.6879554222  0.7918251460  0.7291041577  13.028338779  0.5083986875  3800          0.0173204803 
0.6359545747  0.6354479211  0.7307012375  0.6510930133  0.7954679381  0.7267466781  13.371189800  0.4989294589  3900          0.0171165705 
0.6945575316  0.6868838405  0.7780575347  0.7025289327  0.7901644613  0.7190312902  13.714040820  0.4920874858  4000          0.0171984363 
0.6586672381  0.6532361766  0.7541651042  0.6607372482  0.7725397761  0.6984569224  14.056891841  0.4924541983  4100          0.0176251316 
0.6850224984  0.6789541363  0.7838967161  0.6862408916  0.7973429046  0.7096013716  14.399742861  0.4832036459  4200          0.0180257177 
0.6294193272  0.6238748393  0.7512187282  0.6506643806  0.8272352279  0.7490355765  14.742593882  0.4628953278  4300          0.0179549146 
0.6769337904  0.6753107587  0.7958429314  0.6810972996  0.8095034017  0.7141020146  15.085444902  0.4476064500  4400          0.0191145682 
0.6585601028  0.6603086155  0.7942358172  0.6742391770  0.8302887448  0.7205315045  15.428295923  0.4480580872  4500          0.0192217875 
0.6438290122  0.6455207887  0.7973429046  0.6528075439  0.8177532544  0.6999571367  15.771146943  0.4312528014  4600          0.0191369867 
0.6729162203  0.6735962280  0.8183425296  0.6828118303  0.8254138319  0.6993141877  16.113997964  0.4368291533  4700          0.0192269087 
0.6435611742  0.6405915131  0.8154497241  0.6579511359  0.8368779129  0.6950278611  16.456848984  0.4145879886  4800          0.0192211652 
0.6485429612  0.6478782683  0.8206996304  0.6680240034  0.8347350940  0.6971710244  16.799700005  0.4022316468  4900          0.0177412295 
0.6413113349  0.6478782683  0.8311994429  0.6607372482  0.8546633096  0.7091727390  17.142551025  0.3874091733  5000          0.0178632641 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: AnticausalReg
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/28f6badc4154f4f919f15760cd7debf8
	save_model_every_checkpoint: False
	seed: 671640484
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.4922862653  0.5049292756  0.5017410403  0.4922846121  0.5078480741  0.5111444492  0.0000000000  461.95614624  0             0.2308492661 
0.4965716735  0.5085726532  0.5034017250  0.4920702958  0.4942947447  0.4984997857  0.3428510205  20.042202321  100           0.0259766579 
0.4971073495  0.5094299186  0.5044195639  0.4922846121  0.4932769058  0.4967852550  0.6857020410  1.5544431257  200           0.0252622533 
0.7057531605  0.7209601372  0.6674345101  0.6654522075  0.3512615846  0.3452636091  1.0285530616  1.4804548836  300           0.0257785606 
0.8736875937  0.8671238748  0.7853431189  0.7869695671  0.1468902341  0.1423060437  1.3714040821  1.4503425467  400           0.0245874333 
0.8740089994  0.8722674668  0.7902716023  0.7912558937  0.1513901537  0.1390912988  1.7142551026  1.4273556876  500           0.0256904626 
0.8639918577  0.8628375482  0.7819146087  0.7854693528  0.1635506509  0.1502357480  2.0571061231  1.4059503078  600           0.0264689517 
0.8958110135  0.8919845692  0.8017356833  0.8039005572  0.1055874002  0.0985855122  2.3999571436  1.3798965228  700           0.0263863707 
0.8977930148  0.8939134162  0.8025928108  0.8060437205  0.1031767290  0.0957993999  2.7428081641  1.3507012057  800           0.0270918250 
0.8981144204  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0955850836  3.0856591847  1.3228269565  900           0.0263216090 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0955850836  3.4285102052  1.2837032032  1000          0.0263959026 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0955850836  3.7713612257  1.2318587732  1100          0.0264051104 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0955850836  4.1142122462  1.1917125988  1200          0.0274300694 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0955850836  4.4570632667  1.1757016599  1300          0.0264376545 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0955850836  4.7999142872  1.1364218187  1400          0.0263273478 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1028553062  0.0955850836  5.1427653078  1.1196687543  1500          0.0259731507 
0.8980072852  0.8945563652  0.8026999518  0.8060437205  0.1030160176  0.0957993999  5.4856163283  1.0982496822  1600          0.0261970711 
0.8979537176  0.8945563652  0.8026999518  0.8060437205  0.1031767290  0.0957993999  5.8284673488  1.0855796200  1700          0.0266094327 
0.8981679880  0.8943420489  0.8027535223  0.8062580369  0.1029088766  0.0957993999  6.1713183693  1.0563869786  1800          0.0257338381 
0.8981144204  0.8943420489  0.8027535223  0.8062580369  0.1028553062  0.0957993999  6.5141693898  1.0415778393  1900          0.0269309282 
0.8979537176  0.8945563652  0.8026999518  0.8060437205  0.1032302995  0.0962280326  6.8570204103  1.0457255834  2000          0.0260440087 
0.8980608528  0.8941277325  0.8028070927  0.8058294042  0.1032302995  0.0960137162  7.1998714309  1.0341968393  2100          0.0266917205 
0.8981144204  0.8943420489  0.8027535223  0.8062580369  0.1030160176  0.0957993999  7.5427224514  1.0239641947  2200          0.0277556968 
0.8981144204  0.8941277325  0.8028070927  0.8058294042  0.1031231585  0.0957993999  7.8855734719  1.0141449076  2300          0.0264508677 
0.8981144204  0.8941277325  0.8026999518  0.8062580369  0.1030695880  0.0957993999  8.2284244924  1.0041720897  2400          0.0285798740 
0.8981144204  0.8941277325  0.8027535223  0.8060437205  0.1031231585  0.0957993999  8.5712755129  1.0044470942  2500          0.0262823009 
0.8979001500  0.8941277325  0.8026463813  0.8058294042  0.1033374404  0.0962280326  8.9141265335  1.0071896768  2600          0.0271985769 
0.8978465824  0.8941277325  0.8026463813  0.8056150879  0.1035517223  0.0964423489  9.2569775540  0.9496189445  2700          0.0267171884 
0.8979001500  0.8943420489  0.8026999518  0.8060437205  0.1036052928  0.0964423489  9.5998285745  0.9647995657  2800          0.0263488793 
0.8979001500  0.8941277325  0.8025928108  0.8056150879  0.1036588632  0.0966566652  9.9426795950  0.9481287140  2900          0.0264238620 
0.8979537176  0.8939134162  0.8025392404  0.8056150879  0.1036588632  0.0968709816  10.285530615  0.9571162993  3000          0.0267317629 
0.8979001500  0.8939134162  0.8023785290  0.8056150879  0.1038195747  0.0966566652  10.628381636  0.9533709311  3100          0.0244720817 
0.8979001500  0.8939134162  0.8023785290  0.8058294042  0.1036588632  0.0964423489  10.971232656  0.9366432190  3200          0.0277175283 
0.8981144204  0.8943420489  0.8026999518  0.8060437205  0.1030160176  0.0957993999  11.314083677  0.9272298610  3300          0.0272242188 
0.8975251768  0.8934847835  0.8021106766  0.8049721389  0.1048374136  0.0985855122  11.656934697  0.9200063658  3400          0.0259405565 
0.8974180416  0.8934847835  0.8033963679  0.8056150879  0.1064980982  0.0998714102  11.999785718  0.9329019916  3500          0.0273037505 
0.8980072852  0.8939134162  0.8024856699  0.8058294042  0.1035517223  0.0964423489  12.342636738  0.9397731501  3600          0.0253918695 
0.8977930148  0.8941277325  0.8026999518  0.8058294042  0.1036588632  0.0966566652  12.685487759  0.9369101918  3700          0.0265077877 
0.8973644740  0.8939134162  0.8016285423  0.8043291899  0.1062838164  0.0996570939  13.028338779  0.9019264603  3800          0.0265346360 
0.8975787444  0.8936990999  0.8029678041  0.8060437205  0.1052124069  0.0977282469  13.371189800  0.9143252814  3900          0.0264348865 
0.8966680951  0.8928418345  0.8032892270  0.8056150879  0.1088016285  0.1030861552  13.714040820  0.9181639600  4000          0.0260351491 
0.8976323120  0.8941277325  0.8025392404  0.8056150879  0.1041945680  0.0970852979  14.056891841  0.9124976724  4100          0.0258276153 
0.8802764088  0.8724817831  0.7931644078  0.7944706387  0.1715326512  0.1588084012  14.399742861  0.8711199492  4200          0.0265699100 
0.8922219841  0.8883411916  0.7984143140  0.7996142306  0.1268013071  0.1163737677  14.742593882  0.9127510291  4300          0.0258496189 
0.8975787444  0.8941277325  0.8028070927  0.8060437205  0.1039267156  0.0970852979  15.085444902  0.9015427780  4400          0.0270777082 
0.8973644740  0.8939134162  0.8028606632  0.8064723532  0.1056945412  0.0990141449  15.428295923  0.8996649492  4500          0.0256937504 
0.8968287979  0.8936990999  0.8033963679  0.8062580369  0.1072480849  0.1018002572  15.771146943  0.8922167528  4600          0.0272566724 
0.8973109064  0.8939134162  0.8018428242  0.8041148736  0.1063909573  0.1007286755  16.113997964  0.9005332351  4700          0.0267775106 
0.8968823655  0.8928418345  0.8035035089  0.8058294042  0.1080516419  0.1020145735  16.456848984  0.9028240681  4800          0.0254093480 
0.8979537176  0.8941277325  0.8026463813  0.8056150879  0.1040338565  0.0972996142  16.799700005  0.8923236513  4900          0.0272414088 
0.8980072852  0.8943420489  0.8027535223  0.8064723532  0.1042481384  0.0975139306  17.142551025  0.8823821515  5000          0.0231073928 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/2187750a753111b1a542727da09eded1
	save_model_every_checkpoint: False
	seed: 1552036488
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          nll           penalty       step          step_time    
0.8889007928  0.8915559366  0.7923608507  0.7959708530  0.1011410511  0.1030861552  0.0000000000  0.7887982726  0.7800300717  0.0087681822  0             0.3479714394 
0.8974180416  0.9029147021  0.8002357101  0.8024003429  0.0977661113  0.0994427775  0.3428510205  0.4547254229  0.4394625825  0.0152628415  100           0.0215029836 
0.8974180416  0.9029147021  0.8002357101  0.8024003429  0.0977661113  0.0994427775  0.6857020410  0.4383469909  0.4318748719  0.0064721203  200           0.0217699814 
0.8974180416  0.9029147021  0.8002357101  0.8024003429  0.0977661113  0.0994427775  1.0285530616  0.4343850783  0.4280294067  0.0063556713  300           0.0219501019 
0.8974180416  0.9029147021  0.8002357101  0.8024003429  0.0977661113  0.0994427775  1.3714040821  0.4373258105  0.4312752706  0.0060505409  400           0.0209260035 
0.8974180416  0.9029147021  0.8002357101  0.8024003429  0.0977661113  0.0994427775  1.7142551026  0.4311268839  0.4141589338  0.0066236621  500           0.0226393414 
0.5557638740  0.5653664809  0.5436867199  0.5402914702  0.4467777361  0.4404200600  2.0571061231  0.8210741553  0.5345812070  0.0028649295  600           0.0216655803 
0.6117955860  0.6215173596  0.5869716612  0.5756536648  0.3844752772  0.3836262323  2.3999571436  0.7192820314  0.5772700271  0.0014201200  700           0.0215241933 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  2.7428081641  0.7774552128  0.6041835105  0.0017327170  800           0.0217690206 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  3.0856591847  0.7011391783  0.6307802492  0.0007035893  900           0.0220831609 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  3.4285102052  0.6368350643  0.6215838677  0.0001525120  1000          0.0222238779 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  3.7713612257  0.6879823384  0.6144012952  0.0007358104  1100          0.0231161571 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  4.1142122462  0.6920831403  0.6487652254  0.0004331792  1200          0.0235465479 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  4.4570632667  0.6993230832  0.6698949325  0.0002942815  1300          0.0223242521 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  4.7999142872  0.6846911973  0.6749985331  0.0000969267  1400          0.0226260638 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  5.1427653078  0.6968715903  0.6536584967  0.0004321309  1500          0.0233570313 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  5.4856163283  0.6821876037  0.6508838588  0.0003130375  1600          0.0225237775 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  5.8284673488  0.6979995954  0.6563952947  0.0004160430  1700          0.0233270097 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  6.1713183693  0.7006979445  0.6568169814  0.0004388096  1800          0.0228980184 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  6.5141693898  0.6676034606  0.6583690375  0.0000923442  1900          0.0230356216 
0.5087850868  0.5184312045  0.5064016714  0.5012858980  0.5062409600  0.5038576940  6.8570204103  0.6787200713  0.6524754697  0.0002624460  2000          0.0227496862 
0.5009642168  0.4929275611  0.5031874431  0.5062151736  0.4897948251  0.4927132447  7.1998714309  0.7802684021  0.6484668881  0.0013180151  2100          0.0221965170 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  7.5427224514  0.6947394133  0.6801650095  0.0001457441  2200          0.0224217296 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  7.8855734719  0.6972789979  0.6781666440  0.0001911235  2300          0.0226072884 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  8.2284244924  0.6939400345  0.6490430588  0.0004489697  2400          0.0227080441 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  8.5712755129  0.7053988063  0.6752135611  0.0003018525  2500          0.0232173920 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  8.9141265335  0.6763933742  0.6640816617  0.0001231171  2600          0.0227506113 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  9.2569775540  0.6864531940  0.6552816087  0.0003117158  2700          0.0227350974 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  9.5998285745  0.6787510762  0.6496751785  0.0002907590  2800          0.0226474309 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  9.9426795950  0.6847545007  0.6486379641  0.0003611654  2900          0.0225799513 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  10.285530615  0.7219862348  0.6439680827  0.0007801815  3000          0.0221088767 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  10.628381636  0.6925570115  0.6516846949  0.0004087231  3100          0.0221751046 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  10.971232656  0.6723590285  0.6537145591  0.0001864447  3200          0.0221953082 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  11.314083677  0.6999770665  0.6721557057  0.0002782136  3300          0.0227277303 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  11.656934697  0.6988383263  0.6783463347  0.0002049199  3400          0.0226429176 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  11.999785718  0.6911613315  0.6843119293  0.0000684940  3500          0.0231030321 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  12.342636738  0.6776580960  0.6779507720  -0.000002926  3600          0.0246389461 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  12.685487759  0.6814347288  0.6578049910  0.0002362974  3700          0.0242014885 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  13.028338779  0.6788243878  0.6577852458  0.0002103914  3800          0.0234998870 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  13.371189800  0.6524529091  0.6501610076  0.0000229190  3900          0.0250904608 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  13.714040820  0.6758377391  0.6534241992  0.0002241354  4000          0.0248816204 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  14.056891841  0.6768140414  0.6467779744  0.0003003606  4100          0.0256990123 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  14.399742861  0.6540457979  0.6272632802  0.0002678252  4200          0.0246635747 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  14.742593882  0.7025167400  0.6249201632  0.0007759658  4300          0.0250080419 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  15.085444902  0.6747682008  0.6530911124  0.0002167708  4400          0.0249548125 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  15.428295923  0.6780435896  0.6507814634  0.0002726213  4500          0.0246594954 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  15.771146943  0.6506408226  0.6407009953  0.0000993983  4600          0.0254757547 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  16.113997964  0.6972073266  0.6350204909  0.0006218683  4700          0.0261596966 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  16.456848984  0.6776597959  0.6509566009  0.0002670319  4800          0.0251563334 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  16.799700005  0.6939507332  0.6308491021  0.0006310163  4900          0.0245513511 
0.4912149132  0.4815687955  0.4935983286  0.4987141020  0.4937590400  0.4961423060  17.142551025  0.6726258433  0.6511716282  0.0002145422  5000          0.0265367436 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/bff365b544e8d9cdb61b1b91452f7d54
	save_model_every_checkpoint: False
	seed: 1917354402
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          nll           penalty       step          step_time    
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  0.0000000000  0.6830755472  0.6812856197  0.0017898981  0             0.2440483570 
0.3827405185  0.3954136305  0.4137783254  0.4204886412  0.6214174747  0.6033004715  0.3428510205  0.7760654944  0.7125693810  0.0634961103  100           0.0247152400 
0.3505463895  0.3572653236  0.3903144587  0.3891984569  0.6671130873  0.6519502786  0.6857020410  0.6922502577  0.6909703648  0.0012798923  200           0.0245793104 
0.1113134776  0.1035147878  0.2065677399  0.2102443206  0.8980553919  0.8934847835  1.0285530616  0.6892136550  0.6878126681  0.0014009865  300           0.0241298270 
0.3803299764  0.3780540077  0.4326351315  0.4243463352  0.7642899234  0.7674667810  1.3714040821  0.6845484912  0.6827535582  0.0017949329  400           0.0235362649 
0.6600064281  0.6720960137  0.6391493009  0.6442348907  0.4529383404  0.4384912130  1.7142551026  0.6717495656  0.6672013420  0.0018330615  500           0.0246297622 
0.4557531605  0.4665666524  0.4851877645  0.4886412345  0.6259173943  0.6215173596  2.0571061231  0.7357064408  0.6931632781  0.0004254317  600           0.0244423485 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  2.3999571436  0.6917831415  0.6906601745  0.0000112297  700           0.0237498212 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  2.7428081641  0.7040293550  0.6953909266  0.0000863843  800           0.0241569138 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  3.0856591847  0.6932992721  0.6934034222  -0.000001041  900           0.0251966619 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  3.4285102052  0.6935608315  0.6937478197  -0.000001869  1000          0.0254012036 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  3.7713612257  0.6932970512  0.6933534271  -0.000000563  1100          0.0240683699 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  4.1142122462  0.6937548065  0.6937700212  -0.000000152  1200          0.0242840028 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  4.4570632667  0.6983208096  0.6945379251  0.0000378289  1300          0.0210139775 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  4.7999142872  0.6932370675  0.6931842917  0.0000005278  1400          0.0205017328 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  5.1427653078  0.6932873136  0.6932069916  0.0000008032  1500          0.0219554901 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  5.4856163283  0.6938233256  0.6932666039  0.0000055672  1600          0.0212321353 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  5.8284673488  0.6935521030  0.6933663744  0.0000018573  1700          0.0205795908 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  6.1713183693  0.7048085040  0.6966029805  0.0000820552  1800          0.0211054945 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  6.5141693898  0.6966003573  0.6947456259  0.0000185473  1900          0.0207335424 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  6.8570204103  0.6939400595  0.6932069778  0.0000073308  2000          0.0223094201 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  7.1998714309  0.6960505658  0.6944797701  0.0000157080  2100          0.0209019184 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  7.5427224514  0.6935096478  0.6928284407  0.0000068121  2200          0.0206599021 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  7.8855734719  0.6991612601  0.6936481309  0.0000551313  2300          0.0209943104 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  8.2284244924  0.6936066419  0.6929231691  0.0000068348  2400          0.0211795807 
0.5234090422  0.5255036434  0.5227942358  0.5235747964  0.5113301548  0.4954993571  8.5712755129  0.6914723766  0.6908400714  0.0000063230  2500          0.0210922503 
0.5800299979  0.5724389198  0.5774896877  0.5696528075  0.5887930573  0.5970852979  8.9141265335  0.6952272642  0.6920613796  0.0000316589  2600          0.0209295964 
0.4900364260  0.4976425204  0.4977232549  0.5053579083  0.5000267852  0.4800685812  9.2569775540  0.6911594093  0.6898889142  0.0000127049  2700          0.0212030625 
0.6428647954  0.6397342477  0.6342743880  0.6393056151  0.5817217550  0.5812258894  9.5998285745  0.6905219579  0.6879011416  0.0000262082  2800          0.0202095842 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  9.9426795950  0.6919761610  0.6882136059  0.0000376255  2900          0.0235443401 
0.5280694236  0.5332190313  0.5301333905  0.5353621946  0.5367761290  0.5227175311  10.285530615  0.6941224045  0.6891467589  0.0000497564  3000          0.0229993701 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  10.628381636  0.6922500914  0.6877731615  0.0000447693  3100          0.0232220364 
0.5400149989  0.5480068581  0.5434188675  0.5555079297  0.5197407189  0.5036433776  10.971232656  0.6942374295  0.6841449434  0.0001009249  3200          0.0231788158 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  11.314083677  0.6911961108  0.6894171101  0.0000177900  3300          0.0222027278 
0.5388900793  0.5428632662  0.5420796057  0.5475782255  0.5571864788  0.5443634805  11.656934697  0.6933967727  0.6893021023  0.0000409467  3400          0.0222167182 
0.5300514249  0.5351478783  0.5297583972  0.5387912559  0.5351154444  0.5167166738  11.999785718  0.6897875208  0.6856828201  0.0000410470  3500          0.0222569966 
0.5406042426  0.5450064295  0.5413831896  0.5477925418  0.5455081159  0.5327903986  12.342636738  0.6900907439  0.6842507845  0.0000583996  3600          0.0215878940 
0.5452646240  0.5490784398  0.5455081159  0.5522931847  0.5550436599  0.5413630519  12.685487759  0.6908348107  0.6867336327  0.0000410118  3700          0.0240754676 
0.5306406685  0.5362194599  0.5374189747  0.5426489498  0.5748111641  0.5567938277  13.028338779  0.6856438988  0.6862552118  -0.000006113  3800          0.0257570314 
0.5521212771  0.5600085727  0.5577757540  0.5679382769  0.5773289763  0.5610801543  13.371189800  0.6853196776  0.6804410982  0.0000487858  3900          0.0235565090 
0.5099635740  0.5023574796  0.5022767451  0.4946420917  0.4999732148  0.5199314188  13.714040820  0.6948793042  0.6846569818  0.0001022232  4000          0.0248969817 
0.5435504607  0.5492927561  0.5460973911  0.5552936134  0.5601864252  0.5450064295  14.056891841  0.6886396194  0.6791425920  0.0000949702  4100          0.0242166162 
0.5946539533  0.6018002572  0.5865430974  0.5882983283  0.5432581561  0.5319331333  14.399742861  0.6805335903  0.6780062371  0.0000252736  4200          0.0231124163 
0.5466573816  0.5505786541  0.5477045053  0.5548649807  0.5552579418  0.5422203172  14.742593882  0.6861892831  0.6806212586  0.0000556802  4300          0.0230639696 
0.5493357617  0.5516502357  0.5499008946  0.5563651950  0.5576686131  0.5415773682  15.085444902  0.6911301661  0.6870145023  0.0000411567  4400          0.0220260358 
0.5460145704  0.5501500214  0.5475437939  0.5550792970  0.5556329351  0.5428632662  15.428295923  0.6891734320  0.6874389559  0.0000173447  4500          0.0221255898 
0.5896185987  0.6018002572  0.5869180907  0.6013716245  0.6081319976  0.5902271753  15.771146943  0.6891359228  0.6838563269  0.0000527959  4600          0.0222101521 
0.5400149989  0.5443634805  0.5437938608  0.5525075011  0.5622756736  0.5465066438  16.113997964  0.6909457058  0.6805224276  0.0001042328  4700          0.0217183399 
0.5439790015  0.5477925418  0.5434724380  0.5533647664  0.5516151497  0.5381483069  16.456848984  0.6867855328  0.6770736998  0.0000971183  4800          0.0216787982 
0.5666380973  0.5784397771  0.5687041303  0.5788684098  0.5825788825  0.5705100729  16.799700005  0.6676041195  0.6783899331  -0.000107858  4900          0.0222786498 
0.5912791943  0.5945135019  0.5989714469  0.6112301757  0.6436492205  0.6326618088  17.142551025  0.6843516278  0.6771913224  0.0000716031  5000          0.0220400262 
Environment:
	Python: 3.8.5
	PyTorch: 1.9.0
	Torchvision: 0.10.0
	CUDA: 10.2
	CUDNN: 7605
	NumPy: 1.21.0
	PIL: 8.2.0
Args:
	algorithm: AnticausalReg
	checkpoint_freq: None
	data_dir: ./domainbed/data/MNIST/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output/f825dba6924b0f62aa92d6e561a9bd48
	save_model_every_checkpoint: False
	seed: 664399803
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
/root/miniconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448234945/work/torch/csrc/utils/tensor_numpy.cpp:180.)
  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          step          step_time    
0.4958217270  0.5008572653  0.4956340065  0.4890698671  0.4965982750  0.4916416631  0.0000000000  369.61773681  0             0.2269423008 
0.6078315835  0.6003000429  0.5907751647  0.5932276039  0.5296512562  0.5422203172  0.3428510205  13.847214767  100           0.0250884080 
0.5039640026  0.4976425204  0.5053838324  0.5092156022  0.5087052017  0.5139305615  0.6857020410  1.5721844661  200           0.0258176589 
0.5042318406  0.4993570510  0.5043659935  0.5109301329  0.5035624364  0.5085726532  1.0285530616  1.4952085769  300           0.0263250613 
0.5103921148  0.5068581226  0.5127229871  0.5177882555  0.5076337923  0.5122160309  1.3714040821  1.4642419744  400           0.0260619998 
0.5364795372  0.5315045006  0.5413831896  0.5458636948  0.5602935662  0.5790827261  1.7142551026  1.4420095623  500           0.0261060238 
0.5581744161  0.5535790827  0.5603471367  0.5638662666  0.5459366797  0.5660094299  2.0571061231  1.4300525749  600           0.0242891288 
0.5858152989  0.5810115731  0.5862216746  0.5835833691  0.5843467081  0.5923703386  2.3999571436  1.4206310534  700           0.0240876460 
0.5585493893  0.5550792970  0.5606149890  0.5657951136  0.5415974715  0.5619374196  2.7428081641  1.4124956572  800           0.0261160040 
0.5595671738  0.5522931847  0.5630792307  0.5717959709  0.5533829753  0.5726532362  3.0856591847  1.4069116545  900           0.0304372478 
0.5942254125  0.5882983283  0.5939358226  0.5987998285  0.6032035142  0.6153021860  3.4285102052  1.4016278350  1000          0.0306749010 
0.6093314763  0.6065152165  0.6047034874  0.6067295328  0.5788825200  0.5863694814  3.7713612257  1.3979932296  1100          0.0303551292 
0.5925112492  0.5848692670  0.5935072588  0.6039434205  0.6215246156  0.6315902272  4.1142122462  1.3968857491  1200          0.0284641647 
0.6188129419  0.6144449207  0.6134354744  0.6185169310  0.5901858895  0.5998714102  4.4570632667  1.3951733446  1300          0.0294723392 
0.6040818513  0.6018002572  0.6020785343  0.6069438491  0.5820431778  0.5949421346  4.7999142872  1.3924625278  1400          0.0305877662 
0.6244375402  0.6174453493  0.6187389511  0.6260180026  0.5864359565  0.5983711959  5.1427653078  1.3865198565  1500          0.0310958481 
0.5978144418  0.5927989713  0.5982214603  0.6063009001  0.5852574061  0.6018002572  5.4856163283  1.3889411938  1600          0.0308447099 
0.5764409685  0.5754393485  0.5808110569  0.5889412773  0.6010071249  0.6112301757  5.8284673488  1.3867596936  1700          0.0315339708 
0.6145275337  0.6101585941  0.6113462260  0.6086583798  0.5682219960  0.5810115731  6.1713183693  1.3942023671  1800          0.0301169729 
0.5912791943  0.5904414916  0.5940965340  0.6005143592  0.6056141855  0.6129447064  6.5141693898  1.3963669395  1900          0.0294581342 
0.5540497107  0.5430775825  0.5600257138  0.5587226747  0.6101676756  0.6198028290  6.8570204103  1.3863779044  2000          0.0284706283 
0.6047782301  0.6033004715  0.6041677827  0.6088726961  0.5828467349  0.6003000429  7.1998714309  1.3901854515  2100          0.0285566568 
0.6200449968  0.6161594514  0.6162747094  0.6155165024  0.5915787218  0.6090870124  7.5427224514  1.4105077171  2200          0.0266787767 
0.6219734305  0.6161594514  0.6209889109  0.6234462066  0.5983286013  0.6112301757  7.8855734719  1.3892066622  2300          0.0275856709 
0.5528176559  0.5471495928  0.5669898752  0.5653664809  0.6505598114  0.6613801972  8.2284244924  1.3856470490  2400          0.0261994791 
0.6438290122  0.6367338191  0.6410778379  0.6410201457  0.6191675149  0.6324474925  8.5712755129  1.3854963994  2500          0.0268848658 
0.6364902507  0.6335190742  0.6381314619  0.6399485641  0.6247388439  0.6375910844  8.9141265335  1.3788213015  2600          0.0276472735 
0.6600599957  0.6570938706  0.6607382011  0.6654522075  0.6334708309  0.6442348907  9.2569775540  1.3742925668  2700          0.0270425081 
0.6228305121  0.6230175739  0.6312208711  0.6350192885  0.6631488723  0.6748821260  9.5998285745  1.3838429546  2800          0.0253517437 
0.6342404114  0.6350192885  0.6290244817  0.6350192885  0.5850966947  0.6018002572  9.9426795950  1.3805147707  2900          0.0266835523 
0.6664345404  0.6633090441  0.6597203621  0.6620231462  0.6026142390  0.6189455637  10.285530615  1.3603192222  3000          0.0265408969 
0.6686843797  0.6596656665  0.6663631007  0.6757393913  0.6472920126  0.6588084012  10.628381636  1.3614629042  3100          0.0272396779 
0.6806299550  0.6759537077  0.6766486313  0.6868838405  0.6306851663  0.6423060437  10.971232656  1.3674997938  3200          0.0251364970 
0.6108849368  0.6024432062  0.6237210050  0.6245177883  0.6876841485  0.7085297900  11.314083677  1.3626924658  3300          0.0261992550 
0.6455967431  0.6423060437  0.6522204961  0.6521645949  0.6548454492  0.6673810544  11.656934697  1.3642438471  3400          0.0286893129 
0.6543282623  0.6444492070  0.6539347512  0.6577368195  0.6357743612  0.6498071153  11.999785718  1.3413895202  3500          0.0258450389 
0.6982536962  0.6885983712  0.6920769272  0.6945992285  0.6287030589  0.6463780540  12.342636738  1.3633554363  3600          0.0251619458 
0.6683629741  0.6585940849  0.6704344565  0.6675953708  0.6592382279  0.6789541363  12.685487759  1.3561385643  3700          0.0254494953 
0.6950396400  0.6836690956  0.6900412493  0.6911701672  0.6474527241  0.6609515645  13.028338779  1.3303973460  3800          0.0266185784 
0.6733447611  0.6635233605  0.6730058392  0.6768109730  0.6746665238  0.6862408916  13.371189800  1.3355786371  3900          0.0249843621 
0.6108849368  0.6101585941  0.6115605078  0.6185169310  0.6178818235  0.6322331762  13.714040820  1.3303273141  4000          0.0237748575 
0.6823441183  0.6740248607  0.6773450474  0.6791684526  0.6547383082  0.6697385341  14.056891841  1.3333907580  4100          0.0252640080 
0.6804692522  0.6669524218  0.6794342958  0.6810972996  0.6764879199  0.6864552079  14.399742861  1.3279213810  4200          0.0270922494 
0.6825048211  0.6727389627  0.6763807789  0.6791684526  0.6417742540  0.6594513502  14.742593882  1.3156768906  4300          0.0255979061 
0.6155453182  0.6065152165  0.6213103337  0.6240891556  0.6666309530  0.6806686670  15.085444902  1.3367710698  4400          0.0267498136 
0.6997000214  0.6868838405  0.6963625650  0.6948135448  0.6505598114  0.6637376768  15.428295923  1.3152121758  4500          0.0271431899 
0.6755410328  0.6628804115  0.6741843896  0.6774539220  0.6872555847  0.6952421775  15.771146943  1.3352059829  4600          0.0262960649 
0.5982429826  0.5977282469  0.5985428832  0.5947278183  0.5937751112  0.6024432062  16.113997964  1.3236380744  4700          0.0289104342 
0.6802549818  0.6716673811  0.6790057320  0.6845263609  0.6897733969  0.6954564938  16.456848984  1.3434995258  4800          0.0286614037 
0.6911827727  0.6793827690  0.6897198264  0.6973853408  0.6911662292  0.7053150450  16.799700005  1.2822568905  4900          0.0286716914 
0.6832011999  0.6693099014  0.6833449403  0.6849549936  0.7007017732  0.7113159023  17.142551025  1.2904968452  5000          0.0281364870 
Launching...
Making job directories:
Launched 18 jobs!
